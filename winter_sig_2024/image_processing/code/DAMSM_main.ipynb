{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from miscc.utils import mkdir_p\n",
    "from miscc.utils import build_super_images\n",
    "from miscc.losses import sent_loss, words_loss\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "from DAMSM_train import train, evaluate, build_models, parse_args\n",
    "from datasets import TextDataset\n",
    "from datasets import prepare_data\n",
    "\n",
    "from model import RNN_ENCODER, CNN_ENCODER\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "# 파일 경로 조작 및 시스템 경로 설정\n",
    "import time\n",
    "# 코드 실행 시간을 측정\n",
    "import random\n",
    "# 난수 생성\n",
    "import pprint\n",
    "# 설정 파일을 예쁘게 출력하는 용도\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "# 현재 시간과 타임스탬프를 관리\n",
    "import argparse\n",
    "import numpy as np\n",
    "# 배열 연산을 위해 사용\n",
    "from PIL import Image\n",
    "# 이미지 저장 및 변환\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 기본적인 PyTorch 기능과 신경망 모듈\n",
    "import torch.optim as optim\n",
    "# 옵티마이저 관련 모듈\n",
    "from torch.autograd import Variable\n",
    "# 자동 미분 기능을 위한 래퍼 (PyTorch 최신 버전에서는 필요 X)\n",
    "import torch.backends.cudnn as cudnn\n",
    "# GPU 연산 최적화\n",
    "import torchvision.transforms as transforms\n",
    "# 이미지 전처리(transform) 관련 기능 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blues\\바탕 화면\\Hyewon\\Study\\SIG\\2024-SIG\\2024_winter_sig\\image_processing\\code\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.abspath(os.path.join(os.getcwd(), './.'))\n",
    "# .../image_processing/code\n",
    "sys.path.append(dir_path)\n",
    "\n",
    "print(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cfg_file='cfg/DAMSM/coco.yml', gpu_id=0, data_dir='', manualSeed=None)\n"
     ]
    }
   ],
   "source": [
    "# sys.argv = ['pretrain_DAMSM.py', '--cfg', 'cfg/DAMSM/coco.yml', '--gpu', '0'] 맨 앞의 parameter 어떻게 설정해두는 게 맞는 지 확인하기기\n",
    "sys.argv = ['DAMSM_train.py', '--cfg', 'cfg/DAMSM/coco.yml', '--gpu', '0']\n",
    "\n",
    "args = parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'B_VALIDATION': False,\n",
      " 'CONFIG_NAME': 'DAMSM',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'coco',\n",
      " 'DATA_DIR': '../data/coco',\n",
      " 'GAN': {'B_ATTENTION': True,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 64,\n",
      "         'GF_DIM': 128,\n",
      "         'R_NUM': 2,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': 0,\n",
      " 'RNN_TYPE': 'LSTM',\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 15},\n",
      " 'TRAIN': {'BATCH_SIZE': 48,\n",
      "           'B_NET_D': True,\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'ENCODER_LR': 0.002,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 200,\n",
      "           'NET_E': '',\n",
      "           'NET_G': '',\n",
      "           'RNN_GRAD_CLIP': 0.25,\n",
      "           'SMOOTH': {'GAMMA1': 4.0,\n",
      "                      'GAMMA2': 5.0,\n",
      "                      'GAMMA3': 10.0,\n",
      "                      'LAMBDA': 1.0},\n",
      "           'SNAPSHOT_INTERVAL': 5},\n",
      " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
      " 'WORKERS': 1}\n"
     ]
    }
   ],
   "source": [
    "if args.cfg_file is not None:\n",
    "     cfg_from_file(args.cfg_file)\n",
    "\n",
    "if args.gpu_id == -1:\n",
    "    cfg.CUDA = False\n",
    "else:\n",
    "    cfg.GPU_ID = args.gpu_id\n",
    "\n",
    "if args.data_dir != '': # args.data_dir = ../data/coco\n",
    "    cfg.DATA_DIR = args.data_dir\n",
    "    # DATA_DIR: '../data/coco'\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manualSeed: None\n",
      "Updated manualSeed: 1616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('manualSeed:', args.manualSeed)  # manualSeed 값 출력\n",
    "\n",
    "if not cfg.TRAIN.FLAG:\n",
    "    args.manualSeed = 100\n",
    "# coco.yml에서는 cfg.TRAIN.FLAG = true\n",
    "elif args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "    # 1 ~ 10000 사이로 랜덤 값으로 설정정\n",
    "    \n",
    "print('Updated manualSeed:', args.manualSeed)\n",
    "\n",
    "# seed 값 설정해주어서 동일한 난수를 생성하도록록\n",
    "random.seed(args.manualSeed)\n",
    "np.random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if cfg.CUDA: # CUDA 사용하면 해당 GPU에서도 동일한 난수 생성하도록 설정정\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "# dir_path : c:\\Users\\blues\\바탕 화면\\혜원\\개인공부\\SIG\\2024-SIG\\2024_winter_sig\\image_processing\\code\n",
    "\n",
    "output_dir = '../output/%s_%s_%s' % \\\n",
    "    (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp) \n",
    "# 디렉토리 경로 생성\n",
    "\n",
    "model_dir = os.path.join(output_dir, 'Model')\n",
    "# 여기서 저장된 모델을 어떻게 사용할건지에 대해서 코드보고 이해하기\n",
    "\n",
    "vocadict_dir = os.path.join(cfg.DATA_DIR, 'voca_dictionary')\n",
    "# 단어 사전은 data/coco/vaca_Dictionary으로 해서 중복으로 단어 사전 생성 안하도록 하기\n",
    "\n",
    "image_dir = os.path.join(output_dir, 'Image')\n",
    "# 모델과 이미지 저장용 디렉토리 경로 설정\n",
    "\n",
    "mkdir_p(model_dir)\n",
    "# mkdir_p : 지정된 디렉토리가 존재하지 않으면 생성하는 함수\n",
    "mkdir_p(vocadict_dir)\n",
    "mkdir_p(image_dir)\n",
    "\n",
    "torch.cuda.set_device(cfg.GPU_ID)\n",
    "# 저장된 GPU ID로 GPU를 설정한다.\n",
    "cudnn.benchmark = True\n",
    "# cuDNN의 최적화 활성화화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle file from:  ../data/coco\\voca_dictionary\\captions.pickle\n",
      "<datasets.TextDataset object at 0x000001FDDF706960>\n",
      "27312 5\n",
      "Load pickle file from:  ../data/coco\\voca_dictionary\\captions.pickle\n",
      "<datasets.TextDataset object at 0x000001FDE1AFC650>\n",
      "27312 5\n"
     ]
    }
   ],
   "source": [
    "vocadict_dir = os.path.join(cfg.DATA_DIR, 'voca_dictionary')\n",
    "\n",
    "imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
    "# 입력 이미지의 크기를 결정\n",
    "batch_size = cfg.TRAIN.BATCH_SIZE\n",
    "\n",
    "## 이미지 전처리\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(int(imsize * 76 / 64)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "\n",
    "#### train dataset Load\n",
    "dataset_train = TextDataset(cfg.DATA_DIR, vocadict_dir, 'train',\n",
    "                          base_size=cfg.TREE.BASE_SIZE,\n",
    "                          transform=image_transform)\n",
    "# vocadict_dir = cfg.DATA_DIR\\voca_dictionary\n",
    "print(dataset_train)  # None이면 데이터셋이 생성되지 않은 것\n",
    "print(dataset_train.n_words, dataset_train.embeddings_num)\n",
    "\"\"\"\n",
    "dataset.nwords : 데이터 셋의 단어 수\n",
    "dataset.embeddings_num : 임베딩 크기 \n",
    "\"\"\"\n",
    "assert dataset_train, \"Invalid train Dataset\"\n",
    "# PyThon에서 단순한 존재 검증 즉, True로 평가되는 값인지 확인\n",
    "# None 값, 빈 리스트, 빈 문자열, 0, False 같은 값이면 AssertionError 발생\n",
    "\n",
    "# DataLoader : PyTorch에서 데이터를 배치 단위로 처리하기 위한 도구\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "# drop_last=True : 데이터셋의 크기가 배치 크기로 나누어떨어지지 않는 경우 마지막 배치를 버린다.\n",
    "\n",
    "#### validation dataset Load\n",
    "dataset_val = TextDataset(cfg.DATA_DIR, vocadict_dir, 'val',\n",
    "                        base_size=cfg.TREE.BASE_SIZE,\n",
    "                        transform=image_transform)\n",
    "print(dataset_val)\n",
    "print(dataset_val.n_words, dataset_val.embeddings_num)\n",
    "\n",
    "assert dataset_val, \"Invalid val Dataset\"\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |     0/ 2464 batches | ms/batch 72.28 | s_loss  0.02  0.02 | w_loss  0.03  0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   200/ 2464 batches | ms/batch 1522.12 | s_loss  2.64  2.64 | w_loss  2.82  2.63\n",
      "| epoch   0 |   400/ 2464 batches | ms/batch 1449.57 | s_loss  1.64  1.64 | w_loss  1.68  1.61\n",
      "| epoch   0 |   600/ 2464 batches | ms/batch 1420.95 | s_loss  1.38  1.39 | w_loss  1.39  1.35\n",
      "| epoch   0 |   800/ 2464 batches | ms/batch 1449.84 | s_loss  1.27  1.28 | w_loss  1.26  1.22\n",
      "| epoch   0 |  1000/ 2464 batches | ms/batch 1447.58 | s_loss  1.16  1.18 | w_loss  1.15  1.15\n",
      "| epoch   0 |  1200/ 2464 batches | ms/batch 1429.11 | s_loss  1.11  1.13 | w_loss  1.08  1.08\n",
      "| epoch   0 |  1400/ 2464 batches | ms/batch 1450.60 | s_loss  1.08  1.11 | w_loss  1.05  1.06\n",
      "| epoch   0 |  1600/ 2464 batches | ms/batch 1459.43 | s_loss  1.03  1.05 | w_loss  1.00  1.00\n",
      "| epoch   0 |  1800/ 2464 batches | ms/batch 1462.70 | s_loss  1.01  1.03 | w_loss  0.98  0.98\n",
      "| epoch   0 |  2000/ 2464 batches | ms/batch 1452.44 | s_loss  0.98  1.01 | w_loss  0.92  0.94\n",
      "| epoch   0 |  2200/ 2464 batches | ms/batch 1453.48 | s_loss  0.97  0.99 | w_loss  0.90  0.93\n",
      "| epoch   0 |  2400/ 2464 batches | ms/batch 1464.16 | s_loss  0.95  0.97 | w_loss  0.89  0.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   0 | valid loss  1.91  1.73 | lr 0.00200|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch   1 |     0/ 2464 batches | ms/batch 48.62 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   1 |   200/ 2464 batches | ms/batch 1125.59 | s_loss  0.91  0.94 | w_loss  0.84  0.85\n",
      "| epoch   1 |   400/ 2464 batches | ms/batch 1118.73 | s_loss  0.90  0.92 | w_loss  0.82  0.83\n",
      "| epoch   1 |   600/ 2464 batches | ms/batch 1138.23 | s_loss  0.89  0.92 | w_loss  0.80  0.83\n",
      "| epoch   1 |   800/ 2464 batches | ms/batch 1129.32 | s_loss  0.88  0.91 | w_loss  0.81  0.83\n",
      "| epoch   1 |  1000/ 2464 batches | ms/batch 1173.63 | s_loss  0.87  0.90 | w_loss  0.79  0.81\n",
      "| epoch   1 |  1200/ 2464 batches | ms/batch 1236.13 | s_loss  0.85  0.87 | w_loss  0.77  0.79\n",
      "| epoch   1 |  1400/ 2464 batches | ms/batch 1268.28 | s_loss  0.86  0.89 | w_loss  0.80  0.81\n",
      "| epoch   1 |  1600/ 2464 batches | ms/batch 1356.79 | s_loss  0.85  0.88 | w_loss  0.78  0.79\n",
      "| epoch   1 |  1800/ 2464 batches | ms/batch 1406.30 | s_loss  0.85  0.88 | w_loss  0.77  0.79\n",
      "| epoch   1 |  2000/ 2464 batches | ms/batch 1469.82 | s_loss  0.85  0.88 | w_loss  0.76  0.78\n",
      "| epoch   1 |  2200/ 2464 batches | ms/batch 1459.39 | s_loss  0.83  0.87 | w_loss  0.77  0.79\n",
      "| epoch   1 |  2400/ 2464 batches | ms/batch 1450.70 | s_loss  0.84  0.86 | w_loss  0.75  0.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   1 | valid loss  1.67  1.46 | lr 0.00200|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |     0/ 2464 batches | ms/batch 48.71 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   2 |   200/ 2464 batches | ms/batch 1082.99 | s_loss  0.82  0.85 | w_loss  0.71  0.73\n",
      "| epoch   2 |   400/ 2464 batches | ms/batch 1134.18 | s_loss  0.81  0.83 | w_loss  0.71  0.73\n",
      "| epoch   2 |   600/ 2464 batches | ms/batch 1179.21 | s_loss  0.81  0.84 | w_loss  0.71  0.73\n",
      "| epoch   2 |   800/ 2464 batches | ms/batch 1191.94 | s_loss  0.80  0.82 | w_loss  0.69  0.70\n",
      "| epoch   2 |  1000/ 2464 batches | ms/batch 1258.03 | s_loss  0.80  0.83 | w_loss  0.69  0.72\n",
      "| epoch   2 |  1200/ 2464 batches | ms/batch 1262.86 | s_loss  0.79  0.82 | w_loss  0.70  0.73\n",
      "| epoch   2 |  1400/ 2464 batches | ms/batch 1337.03 | s_loss  0.82  0.85 | w_loss  0.72  0.74\n",
      "| epoch   2 |  1600/ 2464 batches | ms/batch 1418.54 | s_loss  0.79  0.81 | w_loss  0.67  0.69\n",
      "| epoch   2 |  1800/ 2464 batches | ms/batch 1483.42 | s_loss  0.80  0.83 | w_loss  0.69  0.71\n",
      "| epoch   2 |  2000/ 2464 batches | ms/batch 1478.51 | s_loss  0.80  0.83 | w_loss  0.69  0.72\n",
      "| epoch   2 |  2200/ 2464 batches | ms/batch 1468.55 | s_loss  0.78  0.81 | w_loss  0.68  0.70\n",
      "| epoch   2 |  2400/ 2464 batches | ms/batch 1459.12 | s_loss  0.79  0.81 | w_loss  0.67  0.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   2 | valid loss  1.59  1.39 | lr 0.00200|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |     0/ 2464 batches | ms/batch 50.42 | s_loss  0.00  0.01 | w_loss  0.00  0.00\n",
      "| epoch   3 |   200/ 2464 batches | ms/batch 1128.48 | s_loss  0.76  0.79 | w_loss  0.65  0.67\n",
      "| epoch   3 |   400/ 2464 batches | ms/batch 1172.54 | s_loss  0.76  0.79 | w_loss  0.66  0.68\n",
      "| epoch   3 |   600/ 2464 batches | ms/batch 1235.82 | s_loss  0.77  0.80 | w_loss  0.66  0.69\n",
      "| epoch   3 |   800/ 2464 batches | ms/batch 1242.29 | s_loss  0.77  0.79 | w_loss  0.65  0.68\n",
      "| epoch   3 |  1000/ 2464 batches | ms/batch 1291.42 | s_loss  0.79  0.81 | w_loss  0.67  0.69\n",
      "| epoch   3 |  1200/ 2464 batches | ms/batch 1319.45 | s_loss  0.79  0.81 | w_loss  0.67  0.69\n",
      "| epoch   3 |  1400/ 2464 batches | ms/batch 1386.76 | s_loss  0.76  0.79 | w_loss  0.63  0.65\n",
      "| epoch   3 |  1600/ 2464 batches | ms/batch 1384.56 | s_loss  0.76  0.78 | w_loss  0.65  0.66\n",
      "| epoch   3 |  1800/ 2464 batches | ms/batch 1408.30 | s_loss  0.76  0.78 | w_loss  0.65  0.67\n",
      "| epoch   3 |  2000/ 2464 batches | ms/batch 1384.79 | s_loss  0.77  0.79 | w_loss  0.65  0.66\n",
      "| epoch   3 |  2200/ 2464 batches | ms/batch 1424.44 | s_loss  0.76  0.79 | w_loss  0.65  0.67\n",
      "| epoch   3 |  2400/ 2464 batches | ms/batch 1432.72 | s_loss  0.77  0.79 | w_loss  0.66  0.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   3 | valid loss  1.60  1.32 | lr 0.00200|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |     0/ 2464 batches | ms/batch 43.98 | s_loss  0.01  0.01 | w_loss  0.00  0.01\n",
      "| epoch   4 |   200/ 2464 batches | ms/batch 1117.76 | s_loss  0.74  0.76 | w_loss  0.62  0.63\n",
      "| epoch   4 |   400/ 2464 batches | ms/batch 1198.55 | s_loss  0.74  0.76 | w_loss  0.62  0.65\n",
      "| epoch   4 |   600/ 2464 batches | ms/batch 1188.97 | s_loss  0.75  0.77 | w_loss  0.62  0.64\n",
      "| epoch   4 |   800/ 2464 batches | ms/batch 1211.77 | s_loss  0.76  0.78 | w_loss  0.64  0.66\n",
      "| epoch   4 |  1000/ 2464 batches | ms/batch 1255.67 | s_loss  0.74  0.77 | w_loss  0.63  0.64\n",
      "| epoch   4 |  1200/ 2464 batches | ms/batch 1288.43 | s_loss  0.75  0.78 | w_loss  0.63  0.65\n",
      "| epoch   4 |  1400/ 2464 batches | ms/batch 1358.47 | s_loss  0.74  0.77 | w_loss  0.62  0.64\n",
      "| epoch   4 |  1600/ 2464 batches | ms/batch 1396.79 | s_loss  0.77  0.79 | w_loss  0.64  0.66\n",
      "| epoch   4 |  1800/ 2464 batches | ms/batch 1379.41 | s_loss  0.76  0.78 | w_loss  0.64  0.65\n",
      "| epoch   4 |  2000/ 2464 batches | ms/batch 1358.66 | s_loss  0.75  0.78 | w_loss  0.62  0.64\n",
      "| epoch   4 |  2200/ 2464 batches | ms/batch 1373.59 | s_loss  0.74  0.77 | w_loss  0.62  0.64\n",
      "| epoch   4 |  2400/ 2464 batches | ms/batch 1362.02 | s_loss  0.75  0.77 | w_loss  0.62  0.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   4 | valid loss  1.54  1.26 | lr 0.00200|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |     0/ 2464 batches | ms/batch 44.08 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   5 |   200/ 2464 batches | ms/batch 1206.52 | s_loss  0.73  0.76 | w_loss  0.61  0.64\n",
      "| epoch   5 |   400/ 2464 batches | ms/batch 1208.05 | s_loss  0.73  0.75 | w_loss  0.60  0.62\n",
      "| epoch   5 |   600/ 2464 batches | ms/batch 1192.99 | s_loss  0.72  0.75 | w_loss  0.60  0.62\n",
      "| epoch   5 |   800/ 2464 batches | ms/batch 1193.17 | s_loss  0.73  0.75 | w_loss  0.59  0.62\n",
      "| epoch   5 |  1000/ 2464 batches | ms/batch 1231.20 | s_loss  0.72  0.75 | w_loss  0.60  0.63\n",
      "| epoch   5 |  1200/ 2464 batches | ms/batch 1272.40 | s_loss  0.72  0.74 | w_loss  0.58  0.61\n",
      "| epoch   5 |  1400/ 2464 batches | ms/batch 1327.70 | s_loss  0.73  0.76 | w_loss  0.60  0.62\n",
      "| epoch   5 |  1600/ 2464 batches | ms/batch 1368.56 | s_loss  0.73  0.76 | w_loss  0.60  0.63\n",
      "| epoch   5 |  1800/ 2464 batches | ms/batch 1351.50 | s_loss  0.74  0.76 | w_loss  0.61  0.64\n",
      "| epoch   5 |  2000/ 2464 batches | ms/batch 1347.58 | s_loss  0.73  0.75 | w_loss  0.59  0.61\n",
      "| epoch   5 |  2200/ 2464 batches | ms/batch 1356.78 | s_loss  0.74  0.76 | w_loss  0.60  0.63\n",
      "| epoch   5 |  2400/ 2464 batches | ms/batch 1347.11 | s_loss  0.73  0.76 | w_loss  0.60  0.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   5 | valid loss  1.54  1.28 | lr 0.00196|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch   6 |     0/ 2464 batches | ms/batch 42.01 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   6 |   200/ 2464 batches | ms/batch 1106.89 | s_loss  0.72  0.74 | w_loss  0.59  0.61\n",
      "| epoch   6 |   400/ 2464 batches | ms/batch 1178.13 | s_loss  0.71  0.73 | w_loss  0.57  0.59\n",
      "| epoch   6 |   600/ 2464 batches | ms/batch 1174.14 | s_loss  0.73  0.76 | w_loss  0.59  0.62\n",
      "| epoch   6 |   800/ 2464 batches | ms/batch 1184.54 | s_loss  0.72  0.74 | w_loss  0.59  0.60\n",
      "| epoch   6 |  1000/ 2464 batches | ms/batch 1243.22 | s_loss  0.72  0.75 | w_loss  0.59  0.60\n",
      "| epoch   6 |  1200/ 2464 batches | ms/batch 1330.04 | s_loss  0.70  0.73 | w_loss  0.57  0.59\n",
      "| epoch   6 |  1400/ 2464 batches | ms/batch 1346.71 | s_loss  0.73  0.76 | w_loss  0.60  0.62\n",
      "| epoch   6 |  1600/ 2464 batches | ms/batch 1371.51 | s_loss  0.72  0.74 | w_loss  0.59  0.61\n",
      "| epoch   6 |  1800/ 2464 batches | ms/batch 1361.86 | s_loss  0.72  0.74 | w_loss  0.58  0.60\n",
      "| epoch   6 |  2000/ 2464 batches | ms/batch 1375.56 | s_loss  0.72  0.75 | w_loss  0.58  0.61\n",
      "| epoch   6 |  2200/ 2464 batches | ms/batch 1403.95 | s_loss  0.71  0.73 | w_loss  0.58  0.60\n",
      "| epoch   6 |  2400/ 2464 batches | ms/batch 1428.75 | s_loss  0.73  0.76 | w_loss  0.59  0.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   6 | valid loss  1.49  1.24 | lr 0.00196|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |     0/ 2464 batches | ms/batch 46.45 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   7 |   200/ 2464 batches | ms/batch 1204.89 | s_loss  0.71  0.74 | w_loss  0.57  0.60\n",
      "| epoch   7 |   400/ 2464 batches | ms/batch 1222.39 | s_loss  0.71  0.73 | w_loss  0.56  0.58\n",
      "| epoch   7 |   600/ 2464 batches | ms/batch 1251.75 | s_loss  0.71  0.74 | w_loss  0.57  0.59\n",
      "| epoch   7 |   800/ 2464 batches | ms/batch 1302.58 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch   7 |  1000/ 2464 batches | ms/batch 1366.53 | s_loss  0.70  0.73 | w_loss  0.57  0.60\n",
      "| epoch   7 |  1200/ 2464 batches | ms/batch 1395.90 | s_loss  0.72  0.74 | w_loss  0.58  0.60\n",
      "| epoch   7 |  1400/ 2464 batches | ms/batch 1415.28 | s_loss  0.71  0.74 | w_loss  0.58  0.60\n",
      "| epoch   7 |  1600/ 2464 batches | ms/batch 1413.73 | s_loss  0.71  0.74 | w_loss  0.58  0.60\n",
      "| epoch   7 |  1800/ 2464 batches | ms/batch 1400.46 | s_loss  0.72  0.75 | w_loss  0.59  0.62\n",
      "| epoch   7 |  2000/ 2464 batches | ms/batch 1407.94 | s_loss  0.73  0.75 | w_loss  0.58  0.61\n",
      "| epoch   7 |  2200/ 2464 batches | ms/batch 1410.09 | s_loss  0.73  0.75 | w_loss  0.61  0.62\n",
      "| epoch   7 |  2400/ 2464 batches | ms/batch 1413.80 | s_loss  0.71  0.74 | w_loss  0.57  0.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   7 | valid loss  1.54  1.21 | lr 0.00196|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |     0/ 2464 batches | ms/batch 43.35 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   8 |   200/ 2464 batches | ms/batch 1225.10 | s_loss  0.69  0.72 | w_loss  0.55  0.58\n",
      "| epoch   8 |   400/ 2464 batches | ms/batch 1230.94 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch   8 |   600/ 2464 batches | ms/batch 1290.78 | s_loss  0.70  0.72 | w_loss  0.56  0.59\n",
      "| epoch   8 |   800/ 2464 batches | ms/batch 1311.60 | s_loss  0.69  0.72 | w_loss  0.56  0.59\n",
      "| epoch   8 |  1000/ 2464 batches | ms/batch 1357.55 | s_loss  0.71  0.73 | w_loss  0.57  0.59\n",
      "| epoch   8 |  1200/ 2464 batches | ms/batch 1428.77 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch   8 |  1400/ 2464 batches | ms/batch 1400.87 | s_loss  0.69  0.71 | w_loss  0.55  0.58\n",
      "| epoch   8 |  1600/ 2464 batches | ms/batch 1415.86 | s_loss  0.70  0.73 | w_loss  0.57  0.59\n",
      "| epoch   8 |  1800/ 2464 batches | ms/batch 1437.24 | s_loss  0.70  0.72 | w_loss  0.55  0.58\n",
      "| epoch   8 |  2000/ 2464 batches | ms/batch 1413.97 | s_loss  0.71  0.73 | w_loss  0.57  0.59\n",
      "| epoch   8 |  2200/ 2464 batches | ms/batch 1406.29 | s_loss  0.72  0.74 | w_loss  0.58  0.60\n",
      "| epoch   8 |  2400/ 2464 batches | ms/batch 1362.80 | s_loss  0.72  0.74 | w_loss  0.58  0.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   8 | valid loss  1.48  1.19 | lr 0.00196|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |     0/ 2464 batches | ms/batch 42.87 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   9 |   200/ 2464 batches | ms/batch 1240.56 | s_loss  0.71  0.73 | w_loss  0.57  0.59\n",
      "| epoch   9 |   400/ 2464 batches | ms/batch 1290.61 | s_loss  0.70  0.72 | w_loss  0.55  0.58\n",
      "| epoch   9 |   600/ 2464 batches | ms/batch 1325.66 | s_loss  0.70  0.72 | w_loss  0.57  0.57\n",
      "| epoch   9 |   800/ 2464 batches | ms/batch 1391.86 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch   9 |  1000/ 2464 batches | ms/batch 1407.32 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch   9 |  1200/ 2464 batches | ms/batch 1412.33 | s_loss  0.71  0.73 | w_loss  0.57  0.58\n",
      "| epoch   9 |  1400/ 2464 batches | ms/batch 1383.08 | s_loss  0.69  0.72 | w_loss  0.55  0.58\n",
      "| epoch   9 |  1600/ 2464 batches | ms/batch 1412.49 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch   9 |  1800/ 2464 batches | ms/batch 1401.92 | s_loss  0.70  0.73 | w_loss  0.55  0.58\n",
      "| epoch   9 |  2000/ 2464 batches | ms/batch 1393.90 | s_loss  0.70  0.73 | w_loss  0.56  0.59\n",
      "| epoch   9 |  2200/ 2464 batches | ms/batch 1412.64 | s_loss  0.70  0.72 | w_loss  0.55  0.58\n",
      "| epoch   9 |  2400/ 2464 batches | ms/batch 1386.21 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   9 | valid loss  1.41  1.10 | lr 0.00196|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |     0/ 2464 batches | ms/batch 44.98 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  10 |   200/ 2464 batches | ms/batch 1277.06 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  10 |   400/ 2464 batches | ms/batch 1292.90 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  10 |   600/ 2464 batches | ms/batch 1367.74 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  10 |   800/ 2464 batches | ms/batch 1416.45 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  10 |  1000/ 2464 batches | ms/batch 1403.91 | s_loss  0.70  0.73 | w_loss  0.56  0.59\n",
      "| epoch  10 |  1200/ 2464 batches | ms/batch 1408.82 | s_loss  0.70  0.73 | w_loss  0.55  0.57\n",
      "| epoch  10 |  1400/ 2464 batches | ms/batch 1421.84 | s_loss  0.71  0.74 | w_loss  0.56  0.59\n",
      "| epoch  10 |  1600/ 2464 batches | ms/batch 1410.55 | s_loss  0.68  0.71 | w_loss  0.54  0.57\n",
      "| epoch  10 |  1800/ 2464 batches | ms/batch 1412.90 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  10 |  2000/ 2464 batches | ms/batch 1411.25 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  10 |  2200/ 2464 batches | ms/batch 1401.07 | s_loss  0.68  0.70 | w_loss  0.54  0.57\n",
      "| epoch  10 |  2400/ 2464 batches | ms/batch 1393.13 | s_loss  0.68  0.70 | w_loss  0.53  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  10 | valid loss  1.44  1.18 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  11 |     0/ 2464 batches | ms/batch 50.29 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  11 |   200/ 2464 batches | ms/batch 1406.54 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  11 |   400/ 2464 batches | ms/batch 1485.56 | s_loss  0.68  0.71 | w_loss  0.54  0.57\n",
      "| epoch  11 |   600/ 2464 batches | ms/batch 1546.44 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  11 |   800/ 2464 batches | ms/batch 1609.47 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  11 |  1000/ 2464 batches | ms/batch 1616.78 | s_loss  0.67  0.69 | w_loss  0.54  0.55\n",
      "| epoch  11 |  1200/ 2464 batches | ms/batch 1587.29 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  11 |  1400/ 2464 batches | ms/batch 1615.25 | s_loss  0.68  0.71 | w_loss  0.53  0.56\n",
      "| epoch  11 |  1600/ 2464 batches | ms/batch 2004.64 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  11 |  1800/ 2464 batches | ms/batch 2491.26 | s_loss  0.69  0.71 | w_loss  0.53  0.56\n",
      "| epoch  11 |  2000/ 2464 batches | ms/batch 2422.56 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  11 |  2200/ 2464 batches | ms/batch 2377.35 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  11 |  2400/ 2464 batches | ms/batch 2403.46 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  11 | valid loss  1.47  1.16 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |     0/ 2464 batches | ms/batch 82.84 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  12 |   200/ 2464 batches | ms/batch 2106.01 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  12 |   400/ 2464 batches | ms/batch 1793.68 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  12 |   600/ 2464 batches | ms/batch 1835.80 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  12 |   800/ 2464 batches | ms/batch 1852.05 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "text_encoder, image_encoder, labels, start_epoch = build_models(dataset_train, batch_size)\n",
    "para = list(text_encoder.parameters())\n",
    "# text_encoder의 모든 학습 가능 파라미터를 리스트에 추가\n",
    "for v in image_encoder.parameters():\n",
    "    if v.requires_grad:\n",
    "        # requires_grad=False인 경우 역전파에서 제외시킴킴\n",
    "        para.append(v)\n",
    "# optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "\n",
    "# 가장 낮은 검증 손실을 저장\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# 옵티마이저를 한 번만 생성\n",
    "optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.98)\n",
    "\n",
    "try:\n",
    "    lr = cfg.TRAIN.ENCODER_LR # 초기 학습률(lr) 설정\n",
    "    for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
    "        # start_epoch부터 MAX_EPOCH까지 학습 반복\n",
    "        # optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
    "        # Adam 옵티마이저를 사용하여 text/image_encoder를 학습\n",
    "        # 일반적으로는 betas=(0.9, 0.999)를 이용\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        count = train(dataloader_train, image_encoder, text_encoder,\n",
    "                    batch_size, labels, optimizer, epoch,\n",
    "                    dataset_train.ixtoword, image_dir)\n",
    "        # 모델을 한 epoch 동안 학습하는 역할을 함\n",
    "        print('-' * 89)\n",
    "\n",
    "        if len(dataloader_val) > 0:\n",
    "            # 학습한 모델을 검증 데이터에서 평가\n",
    "            # 가중치를 업데이트하지 않고, 현재 모델이 얼마나 잘 동작하는지 평가가\n",
    "            s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
    "                                      text_encoder, batch_size, labels)\n",
    "            val_loss = s_loss + w_loss # 검증 손실 합산\n",
    "            print('| end epoch {:3d} | valid loss {:5.2f} {:5.2f} | lr {:.5f}|'\n",
    "                  .format(epoch, s_loss, w_loss, scheduler.get_last_lr()[0]))\n",
    "            \n",
    "            #### 가장 낮은 검증 손실을 가진 모델 저장\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(image_encoder.state_dict(),\n",
    "                           f'{model_dir}/best_image_encoder.pth')\n",
    "                torch.save(text_encoder.state_dict(),\n",
    "                           f'{model_dir}/best_text_encoder.pth')\n",
    "                print('Best model saved!')\n",
    "            \n",
    "        print('-' * 89)\n",
    "        \n",
    "        scheduler.step() # 스케줄러로 학습률 감소\n",
    "\n",
    "        #if lr > cfg.TRAIN.ENCODER_LR/10.:\n",
    "            # 현재 학습률이 초기 학습률의 1/10 이상이면 lr를 98%로 감소시킴\n",
    "        #    lr *= 0.98\n",
    "\n",
    "        # 모델 저장\n",
    "        if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n",
    "            epoch == cfg.TRAIN.MAX_EPOCH):\n",
    "            # 일정 epoch마다 또는 MAX_EPOCH이 되면 가중치 저장\n",
    "            torch.save(image_encoder.state_dict(),\n",
    "                       '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
    "            torch.save(text_encoder.state_dict(),\n",
    "                       '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
    "            print('Save models.')\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
      "Load  ../output/coco_DAMSM_2025_02_19_00_04_58/Model/text_encoder10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blues\\AppData\\Local\\Temp\\ipykernel_7512\\2472341345.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "C:\\Users\\blues\\AppData\\Local\\Temp\\ipykernel_7512\\2472341345.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  ../output/coco_DAMSM_2025_02_19_00_04_58/Model/image_encoder10.pth\n",
      "start_epoch 11\n",
      "| epoch  11 |     0/ 2464 batches | ms/batch 44.59 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  11 |   200/ 2464 batches | ms/batch 1098.46 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  11 |   400/ 2464 batches | ms/batch 1076.17 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  11 |   600/ 2464 batches | ms/batch 1131.07 | s_loss  0.69  0.71 | w_loss  0.56  0.56\n",
      "| epoch  11 |   800/ 2464 batches | ms/batch 1144.42 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  11 |  1000/ 2464 batches | ms/batch 1216.68 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  11 |  1200/ 2464 batches | ms/batch 1186.81 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  11 |  1400/ 2464 batches | ms/batch 1262.33 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  11 |  1600/ 2464 batches | ms/batch 1269.53 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  11 |  1800/ 2464 batches | ms/batch 1144.60 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  11 |  2000/ 2464 batches | ms/batch 1234.20 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  11 |  2200/ 2464 batches | ms/batch 1417.42 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  11 |  2400/ 2464 batches | ms/batch 1309.30 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  11 | valid loss  1.45  1.13 | lr 0.00192|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |     0/ 2464 batches | ms/batch 43.67 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  12 |   200/ 2464 batches | ms/batch 971.16 | s_loss  0.68  0.71 | w_loss  0.53  0.55\n",
      "| epoch  12 |   400/ 2464 batches | ms/batch 1024.01 | s_loss  0.68  0.71 | w_loss  0.56  0.56\n",
      "| epoch  12 |   600/ 2464 batches | ms/batch 996.09 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  12 |   800/ 2464 batches | ms/batch 1075.34 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  12 |  1000/ 2464 batches | ms/batch 1066.58 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  12 |  1200/ 2464 batches | ms/batch 1085.60 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  12 |  1400/ 2464 batches | ms/batch 1130.50 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  12 |  1600/ 2464 batches | ms/batch 1150.70 | s_loss  0.69  0.72 | w_loss  0.54  0.55\n",
      "| epoch  12 |  1800/ 2464 batches | ms/batch 1190.12 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  12 |  2000/ 2464 batches | ms/batch 1385.58 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  12 |  2200/ 2464 batches | ms/batch 1576.49 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  12 |  2400/ 2464 batches | ms/batch 1540.48 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  12 | valid loss  1.47  1.17 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |     0/ 2464 batches | ms/batch 43.83 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  13 |   200/ 2464 batches | ms/batch 942.27 | s_loss  0.67  0.69 | w_loss  0.52  0.53\n",
      "| epoch  13 |   400/ 2464 batches | ms/batch 1281.34 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  13 |   600/ 2464 batches | ms/batch 2400.60 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  13 |   800/ 2464 batches | ms/batch 2463.46 | s_loss  0.70  0.73 | w_loss  0.57  0.59\n",
      "| epoch  13 |  1000/ 2464 batches | ms/batch 2779.11 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  13 |  1200/ 2464 batches | ms/batch 2687.26 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  13 |  1400/ 2464 batches | ms/batch 2927.26 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  13 |  1600/ 2464 batches | ms/batch 2929.31 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  13 |  1800/ 2464 batches | ms/batch 2311.36 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  13 |  2000/ 2464 batches | ms/batch 1845.14 | s_loss  0.67  0.69 | w_loss  0.53  0.54\n",
      "| epoch  13 |  2200/ 2464 batches | ms/batch 2862.90 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  13 |  2400/ 2464 batches | ms/batch 2836.65 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  13 | valid loss  1.51  1.23 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |     0/ 2464 batches | ms/batch 75.28 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  14 |   200/ 2464 batches | ms/batch 1845.82 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  14 |   400/ 2464 batches | ms/batch 2082.24 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  14 |   600/ 2464 batches | ms/batch 1566.64 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  14 |   800/ 2464 batches | ms/batch 2719.53 | s_loss  0.70  0.72 | w_loss  0.55  0.58\n",
      "| epoch  14 |  1000/ 2464 batches | ms/batch 1937.74 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  14 |  1200/ 2464 batches | ms/batch 1557.03 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  14 |  1400/ 2464 batches | ms/batch 1654.34 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  14 |  1600/ 2464 batches | ms/batch 1638.51 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  14 |  1800/ 2464 batches | ms/batch 1576.63 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  14 |  2000/ 2464 batches | ms/batch 1560.22 | s_loss  0.68  0.71 | w_loss  0.53  0.55\n",
      "| epoch  14 |  2200/ 2464 batches | ms/batch 1541.11 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  14 |  2400/ 2464 batches | ms/batch 1557.07 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  14 | valid loss  1.61  1.28 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |     0/ 2464 batches | ms/batch 49.23 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  15 |   200/ 2464 batches | ms/batch 1235.21 | s_loss  0.71  0.73 | w_loss  0.56  0.57\n",
      "| epoch  15 |   400/ 2464 batches | ms/batch 1105.71 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  15 |   600/ 2464 batches | ms/batch 1527.40 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  15 |   800/ 2464 batches | ms/batch 1620.15 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  15 |  1000/ 2464 batches | ms/batch 1633.03 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  15 |  1200/ 2464 batches | ms/batch 1611.22 | s_loss  0.71  0.73 | w_loss  0.56  0.58\n",
      "| epoch  15 |  1400/ 2464 batches | ms/batch 1539.47 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  15 |  1600/ 2464 batches | ms/batch 1509.88 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  15 |  1800/ 2464 batches | ms/batch 1558.79 | s_loss  0.68  0.70 | w_loss  0.55  0.56\n",
      "| epoch  15 |  2000/ 2464 batches | ms/batch 1621.32 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  15 |  2200/ 2464 batches | ms/batch 1550.40 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  15 |  2400/ 2464 batches | ms/batch 1534.30 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  15 | valid loss  1.54  1.23 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  16 |     0/ 2464 batches | ms/batch 47.31 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  16 |   200/ 2464 batches | ms/batch 1098.82 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  16 |   400/ 2464 batches | ms/batch 1142.65 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  16 |   600/ 2464 batches | ms/batch 1185.63 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  16 |   800/ 2464 batches | ms/batch 1182.83 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  16 |  1000/ 2464 batches | ms/batch 1236.87 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  16 |  1200/ 2464 batches | ms/batch 1282.82 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  16 |  1400/ 2464 batches | ms/batch 1367.51 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  16 |  1600/ 2464 batches | ms/batch 1402.49 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  16 |  1800/ 2464 batches | ms/batch 1458.87 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  16 |  2000/ 2464 batches | ms/batch 1562.18 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  16 |  2200/ 2464 batches | ms/batch 1578.98 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  16 |  2400/ 2464 batches | ms/batch 1577.79 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  16 | valid loss  1.50  1.23 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |     0/ 2464 batches | ms/batch 48.46 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  17 |   200/ 2464 batches | ms/batch 1104.99 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  17 |   400/ 2464 batches | ms/batch 1185.10 | s_loss  0.69  0.71 | w_loss  0.56  0.56\n",
      "| epoch  17 |   600/ 2464 batches | ms/batch 1207.41 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  17 |   800/ 2464 batches | ms/batch 1225.76 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  17 |  1000/ 2464 batches | ms/batch 1299.78 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  17 |  1200/ 2464 batches | ms/batch 1304.45 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  17 |  1400/ 2464 batches | ms/batch 1391.33 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  17 |  1600/ 2464 batches | ms/batch 1433.46 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  17 |  1800/ 2464 batches | ms/batch 1579.80 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  17 |  2000/ 2464 batches | ms/batch 1559.97 | s_loss  0.68  0.70 | w_loss  0.55  0.56\n",
      "| epoch  17 |  2200/ 2464 batches | ms/batch 1562.42 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  17 |  2400/ 2464 batches | ms/batch 1580.12 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  17 | valid loss  1.49  1.22 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |     0/ 2464 batches | ms/batch 48.14 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  18 |   200/ 2464 batches | ms/batch 1161.73 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  18 |   400/ 2464 batches | ms/batch 1218.08 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  18 |   600/ 2464 batches | ms/batch 1245.98 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  18 |   800/ 2464 batches | ms/batch 1286.88 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  18 |  1000/ 2464 batches | ms/batch 1359.91 | s_loss  0.68  0.71 | w_loss  0.55  0.55\n",
      "| epoch  18 |  1200/ 2464 batches | ms/batch 1407.58 | s_loss  0.67  0.69 | w_loss  0.53  0.55\n",
      "| epoch  18 |  1400/ 2464 batches | ms/batch 1466.71 | s_loss  0.71  0.73 | w_loss  0.57  0.60\n",
      "| epoch  18 |  1600/ 2464 batches | ms/batch 1534.35 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  18 |  1800/ 2464 batches | ms/batch 1593.77 | s_loss  0.68  0.71 | w_loss  0.55  0.55\n",
      "| epoch  18 |  2000/ 2464 batches | ms/batch 1561.98 | s_loss  0.70  0.73 | w_loss  0.55  0.57\n",
      "| epoch  18 |  2200/ 2464 batches | ms/batch 1532.28 | s_loss  0.70  0.73 | w_loss  0.56  0.57\n",
      "| epoch  18 |  2400/ 2464 batches | ms/batch 1476.29 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  18 | valid loss  1.51  1.22 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |     0/ 2464 batches | ms/batch 46.68 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  19 |   200/ 2464 batches | ms/batch 1147.45 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  19 |   400/ 2464 batches | ms/batch 1154.50 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  19 |   600/ 2464 batches | ms/batch 1154.00 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  19 |   800/ 2464 batches | ms/batch 1247.51 | s_loss  0.67  0.69 | w_loss  0.53  0.54\n",
      "| epoch  19 |  1000/ 2464 batches | ms/batch 1253.41 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  19 |  1200/ 2464 batches | ms/batch 1303.77 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  19 |  1400/ 2464 batches | ms/batch 1377.83 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  19 |  1600/ 2464 batches | ms/batch 1373.38 | s_loss  0.68  0.70 | w_loss  0.55  0.55\n",
      "| epoch  19 |  1800/ 2464 batches | ms/batch 1343.54 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  19 |  2000/ 2464 batches | ms/batch 1370.59 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  19 |  2200/ 2464 batches | ms/batch 1428.88 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  19 |  2400/ 2464 batches | ms/batch 1530.09 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  19 | valid loss  1.55  1.28 | lr 0.00184|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |     0/ 2464 batches | ms/batch 42.59 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  20 |   200/ 2464 batches | ms/batch 1277.34 | s_loss  0.70  0.73 | w_loss  0.57  0.58\n",
      "| epoch  20 |   400/ 2464 batches | ms/batch 1304.80 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  20 |   600/ 2464 batches | ms/batch 1355.61 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  20 |   800/ 2464 batches | ms/batch 1725.35 | s_loss  0.69  0.72 | w_loss  0.56  0.56\n",
      "| epoch  20 |  1000/ 2464 batches | ms/batch 1455.88 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  20 |  1200/ 2464 batches | ms/batch 3184.40 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  20 |  1400/ 2464 batches | ms/batch 3178.07 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  20 |  1600/ 2464 batches | ms/batch 1954.56 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  20 |  1800/ 2464 batches | ms/batch 1581.34 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  20 |  2000/ 2464 batches | ms/batch 1529.92 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  20 |  2200/ 2464 batches | ms/batch 1526.53 | s_loss  0.70  0.72 | w_loss  0.56  0.56\n",
      "| epoch  20 |  2400/ 2464 batches | ms/batch 1523.28 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  20 | valid loss  1.53  1.28 | lr 0.00184|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  21 |     0/ 2464 batches | ms/batch 47.30 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  21 |   200/ 2464 batches | ms/batch 1122.11 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  21 |   400/ 2464 batches | ms/batch 1131.30 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  21 |   600/ 2464 batches | ms/batch 1153.93 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  21 |   800/ 2464 batches | ms/batch 1173.28 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  21 |  1000/ 2464 batches | ms/batch 1186.00 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  21 |  1200/ 2464 batches | ms/batch 1514.76 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  21 |  1400/ 2464 batches | ms/batch 1501.92 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  21 |  1600/ 2464 batches | ms/batch 1495.63 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  21 |  1800/ 2464 batches | ms/batch 1474.99 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  21 |  2000/ 2464 batches | ms/batch 1450.66 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  21 |  2200/ 2464 batches | ms/batch 1456.97 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  21 |  2400/ 2464 batches | ms/batch 1496.20 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  21 | valid loss  1.47  1.16 | lr 0.00184|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |     0/ 2464 batches | ms/batch 47.88 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  22 |   200/ 2464 batches | ms/batch 1152.54 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  22 |   400/ 2464 batches | ms/batch 1140.73 | s_loss  0.68  0.70 | w_loss  0.55  0.56\n",
      "| epoch  22 |   600/ 2464 batches | ms/batch 1179.12 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  22 |   800/ 2464 batches | ms/batch 1176.27 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  22 |  1000/ 2464 batches | ms/batch 1231.51 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  22 |  1200/ 2464 batches | ms/batch 1287.18 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  22 |  1400/ 2464 batches | ms/batch 1336.37 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  22 |  1600/ 2464 batches | ms/batch 1425.13 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  22 |  1800/ 2464 batches | ms/batch 1490.42 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  22 |  2000/ 2464 batches | ms/batch 1469.68 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  22 |  2200/ 2464 batches | ms/batch 1487.59 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  22 |  2400/ 2464 batches | ms/batch 1484.35 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  22 | valid loss  1.48  1.22 | lr 0.00184|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |     0/ 2464 batches | ms/batch 45.63 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  23 |   200/ 2464 batches | ms/batch 1143.68 | s_loss  0.68  0.71 | w_loss  0.55  0.55\n",
      "| epoch  23 |   400/ 2464 batches | ms/batch 1196.08 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  23 |   600/ 2464 batches | ms/batch 1195.69 | s_loss  0.69  0.70 | w_loss  0.54  0.54\n",
      "| epoch  23 |   800/ 2464 batches | ms/batch 1263.22 | s_loss  0.66  0.69 | w_loss  0.53  0.55\n",
      "| epoch  23 |  1000/ 2464 batches | ms/batch 1312.55 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  23 |  1200/ 2464 batches | ms/batch 1353.11 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  23 |  1400/ 2464 batches | ms/batch 1426.12 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  23 |  1600/ 2464 batches | ms/batch 1499.24 | s_loss  0.71  0.73 | w_loss  0.57  0.59\n",
      "| epoch  23 |  1800/ 2464 batches | ms/batch 1480.73 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  23 |  2000/ 2464 batches | ms/batch 1468.38 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  23 |  2200/ 2464 batches | ms/batch 1497.37 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  23 |  2400/ 2464 batches | ms/batch 1483.11 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  23 | valid loss  1.53  1.24 | lr 0.00184|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |     0/ 2464 batches | ms/batch 45.31 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  24 |   200/ 2464 batches | ms/batch 1191.24 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  24 |   400/ 2464 batches | ms/batch 1258.26 | s_loss  0.69  0.72 | w_loss  0.54  0.57\n",
      "| epoch  24 |   600/ 2464 batches | ms/batch 1304.47 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  24 |   800/ 2464 batches | ms/batch 1324.21 | s_loss  0.67  0.70 | w_loss  0.53  0.54\n",
      "| epoch  24 |  1000/ 2464 batches | ms/batch 1397.68 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  24 |  1200/ 2464 batches | ms/batch 1449.76 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  24 |  1400/ 2464 batches | ms/batch 1489.80 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  24 |  1600/ 2464 batches | ms/batch 1452.89 | s_loss  0.68  0.70 | w_loss  0.52  0.55\n",
      "| epoch  24 |  1800/ 2464 batches | ms/batch 1497.50 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  24 |  2000/ 2464 batches | ms/batch 1469.96 | s_loss  0.67  0.69 | w_loss  0.53  0.54\n",
      "| epoch  24 |  2200/ 2464 batches | ms/batch 1494.89 | s_loss  0.70  0.73 | w_loss  0.56  0.58\n",
      "| epoch  24 |  2400/ 2464 batches | ms/batch 1460.77 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  24 | valid loss  1.47  1.24 | lr 0.00181|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |     0/ 2464 batches | ms/batch 49.25 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  25 |   200/ 2464 batches | ms/batch 1266.42 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  25 |   400/ 2464 batches | ms/batch 1302.51 | s_loss  0.69  0.72 | w_loss  0.56  0.56\n",
      "| epoch  25 |   600/ 2464 batches | ms/batch 1334.90 | s_loss  0.68  0.71 | w_loss  0.55  0.57\n",
      "| epoch  25 |   800/ 2464 batches | ms/batch 1402.47 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  25 |  1000/ 2464 batches | ms/batch 1474.01 | s_loss  0.67  0.70 | w_loss  0.54  0.55\n",
      "| epoch  25 |  1200/ 2464 batches | ms/batch 1456.94 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  25 |  1400/ 2464 batches | ms/batch 1482.51 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  25 |  1600/ 2464 batches | ms/batch 1493.27 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  25 |  1800/ 2464 batches | ms/batch 1487.93 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  25 |  2000/ 2464 batches | ms/batch 1488.23 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  25 |  2200/ 2464 batches | ms/batch 1448.86 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  25 |  2400/ 2464 batches | ms/batch 1468.37 | s_loss  0.68  0.71 | w_loss  0.53  0.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  25 | valid loss  1.50  1.22 | lr 0.00181|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  26 |     0/ 2464 batches | ms/batch 50.68 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  26 |   200/ 2464 batches | ms/batch 1346.62 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  26 |   400/ 2464 batches | ms/batch 1375.82 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  26 |   600/ 2464 batches | ms/batch 1432.66 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  26 |   800/ 2464 batches | ms/batch 1470.16 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  26 |  1000/ 2464 batches | ms/batch 1503.70 | s_loss  0.69  0.71 | w_loss  0.56  0.58\n",
      "| epoch  26 |  1200/ 2464 batches | ms/batch 1491.30 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  26 |  1400/ 2464 batches | ms/batch 1500.13 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  26 |  1600/ 2464 batches | ms/batch 1491.27 | s_loss  0.67  0.70 | w_loss  0.54  0.56\n",
      "| epoch  26 |  1800/ 2464 batches | ms/batch 1471.10 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  26 |  2000/ 2464 batches | ms/batch 1491.71 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  26 |  2200/ 2464 batches | ms/batch 1499.12 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  26 |  2400/ 2464 batches | ms/batch 1508.28 | s_loss  0.70  0.73 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  26 | valid loss  1.48  1.20 | lr 0.00181|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |     0/ 2464 batches | ms/batch 49.01 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  27 |   200/ 2464 batches | ms/batch 1344.39 | s_loss  0.68  0.70 | w_loss  0.55  0.56\n",
      "| epoch  27 |   400/ 2464 batches | ms/batch 1406.81 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  27 |   600/ 2464 batches | ms/batch 1436.65 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  27 |   800/ 2464 batches | ms/batch 1491.53 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  27 |  1000/ 2464 batches | ms/batch 1456.87 | s_loss  0.68  0.70 | w_loss  0.54  0.54\n",
      "| epoch  27 |  1200/ 2464 batches | ms/batch 1500.61 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  27 |  1400/ 2464 batches | ms/batch 1494.84 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  27 |  1600/ 2464 batches | ms/batch 1475.35 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  27 |  1800/ 2464 batches | ms/batch 1523.03 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  27 |  2000/ 2464 batches | ms/batch 1491.38 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  27 |  2200/ 2464 batches | ms/batch 1462.76 | s_loss  0.67  0.70 | w_loss  0.52  0.54\n",
      "| epoch  27 |  2400/ 2464 batches | ms/batch 1470.96 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  27 | valid loss  1.53  1.22 | lr 0.00181|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |     0/ 2464 batches | ms/batch 48.29 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  28 |   200/ 2464 batches | ms/batch 1338.18 | s_loss  0.69  0.71 | w_loss  0.55  0.58\n",
      "| epoch  28 |   400/ 2464 batches | ms/batch 1397.17 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  28 |   600/ 2464 batches | ms/batch 1446.59 | s_loss  0.71  0.73 | w_loss  0.56  0.57\n",
      "| epoch  28 |   800/ 2464 batches | ms/batch 1499.69 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  28 |  1000/ 2464 batches | ms/batch 1501.18 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  28 |  1200/ 2464 batches | ms/batch 1470.45 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  28 |  1400/ 2464 batches | ms/batch 1500.67 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  28 |  1600/ 2464 batches | ms/batch 1473.14 | s_loss  0.70  0.73 | w_loss  0.56  0.58\n",
      "| epoch  28 |  1800/ 2464 batches | ms/batch 1449.69 | s_loss  0.68  0.71 | w_loss  0.53  0.56\n",
      "| epoch  28 |  2000/ 2464 batches | ms/batch 1455.02 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  28 |  2200/ 2464 batches | ms/batch 1499.37 | s_loss  0.69  0.71 | w_loss  0.53  0.55\n",
      "| epoch  28 |  2400/ 2464 batches | ms/batch 1498.41 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  28 | valid loss  1.51  1.18 | lr 0.00181|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |     0/ 2464 batches | ms/batch 47.80 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  29 |   200/ 2464 batches | ms/batch 1422.98 | s_loss  0.72  0.74 | w_loss  0.57  0.58\n",
      "| epoch  29 |   400/ 2464 batches | ms/batch 1411.87 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  29 |   600/ 2464 batches | ms/batch 1470.96 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  29 |   800/ 2464 batches | ms/batch 1456.73 | s_loss  0.70  0.73 | w_loss  0.56  0.57\n",
      "| epoch  29 |  1000/ 2464 batches | ms/batch 1486.26 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  29 |  1200/ 2464 batches | ms/batch 1477.13 | s_loss  0.68  0.71 | w_loss  0.55  0.57\n",
      "| epoch  29 |  1400/ 2464 batches | ms/batch 1489.88 | s_loss  0.70  0.71 | w_loss  0.54  0.56\n",
      "| epoch  29 |  1600/ 2464 batches | ms/batch 1475.80 | s_loss  0.68  0.71 | w_loss  0.55  0.57\n",
      "| epoch  29 |  1800/ 2464 batches | ms/batch 1484.77 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  29 |  2000/ 2464 batches | ms/batch 1473.13 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  29 |  2200/ 2464 batches | ms/batch 1489.49 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  29 |  2400/ 2464 batches | ms/batch 1508.73 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  29 | valid loss  1.52  1.24 | lr 0.00177|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |     0/ 2464 batches | ms/batch 47.52 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  30 |   200/ 2464 batches | ms/batch 1398.23 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  30 |   400/ 2464 batches | ms/batch 1404.91 | s_loss  0.67  0.69 | w_loss  0.53  0.54\n",
      "| epoch  30 |   600/ 2464 batches | ms/batch 1484.20 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  30 |   800/ 2464 batches | ms/batch 1482.81 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  30 |  1000/ 2464 batches | ms/batch 1496.11 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  30 |  1200/ 2464 batches | ms/batch 1485.72 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  30 |  1400/ 2464 batches | ms/batch 1488.92 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  30 |  1600/ 2464 batches | ms/batch 1513.74 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  30 |  1800/ 2464 batches | ms/batch 1475.10 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  30 |  2000/ 2464 batches | ms/batch 1474.92 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  30 |  2200/ 2464 batches | ms/batch 1480.08 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  30 |  2400/ 2464 batches | ms/batch 1450.46 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  30 | valid loss  1.49  1.20 | lr 0.00177|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  31 |     0/ 2464 batches | ms/batch 51.80 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  31 |   200/ 2464 batches | ms/batch 1379.13 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  31 |   400/ 2464 batches | ms/batch 1412.50 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  31 |   600/ 2464 batches | ms/batch 1464.74 | s_loss  0.70  0.73 | w_loss  0.56  0.58\n",
      "| epoch  31 |   800/ 2464 batches | ms/batch 1501.50 | s_loss  0.67  0.69 | w_loss  0.53  0.54\n",
      "| epoch  31 |  1000/ 2464 batches | ms/batch 1450.34 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  31 |  1200/ 2464 batches | ms/batch 1484.96 | s_loss  0.69  0.71 | w_loss  0.53  0.56\n",
      "| epoch  31 |  1400/ 2464 batches | ms/batch 1462.02 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  31 |  1600/ 2464 batches | ms/batch 1474.83 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  31 |  1800/ 2464 batches | ms/batch 1506.42 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  31 |  2000/ 2464 batches | ms/batch 1492.66 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  31 |  2200/ 2464 batches | ms/batch 1493.88 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  31 |  2400/ 2464 batches | ms/batch 1488.20 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  31 | valid loss  1.48  1.20 | lr 0.00177|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |     0/ 2464 batches | ms/batch 48.75 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  32 |   200/ 2464 batches | ms/batch 1374.66 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  32 |   400/ 2464 batches | ms/batch 1393.37 | s_loss  0.69  0.72 | w_loss  0.54  0.57\n",
      "| epoch  32 |   600/ 2464 batches | ms/batch 1491.74 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  32 |   800/ 2464 batches | ms/batch 1503.10 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  32 |  1000/ 2464 batches | ms/batch 1503.45 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  32 |  1200/ 2464 batches | ms/batch 1492.60 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  32 |  1400/ 2464 batches | ms/batch 1519.59 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  32 |  1600/ 2464 batches | ms/batch 1473.33 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  32 |  1800/ 2464 batches | ms/batch 1498.77 | s_loss  0.67  0.70 | w_loss  0.52  0.55\n",
      "| epoch  32 |  2000/ 2464 batches | ms/batch 1508.96 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  32 |  2200/ 2464 batches | ms/batch 1499.92 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  32 |  2400/ 2464 batches | ms/batch 1500.49 | s_loss  0.70  0.72 | w_loss  0.56  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  32 | valid loss  1.50  1.17 | lr 0.00177|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |     0/ 2464 batches | ms/batch 49.10 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  33 |   200/ 2464 batches | ms/batch 1365.42 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  33 |   400/ 2464 batches | ms/batch 1446.73 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  33 |   600/ 2464 batches | ms/batch 1487.82 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  33 |   800/ 2464 batches | ms/batch 1475.70 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  33 |  1000/ 2464 batches | ms/batch 1484.16 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  33 |  1200/ 2464 batches | ms/batch 1501.56 | s_loss  0.70  0.72 | w_loss  0.56  0.56\n",
      "| epoch  33 |  1400/ 2464 batches | ms/batch 1511.24 | s_loss  0.67  0.69 | w_loss  0.53  0.54\n",
      "| epoch  33 |  1600/ 2464 batches | ms/batch 1517.88 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  33 |  1800/ 2464 batches | ms/batch 1497.15 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  33 |  2000/ 2464 batches | ms/batch 1487.80 | s_loss  0.70  0.72 | w_loss  0.54  0.57\n",
      "| epoch  33 |  2200/ 2464 batches | ms/batch 1492.93 | s_loss  0.70  0.73 | w_loss  0.57  0.58\n",
      "| epoch  33 |  2400/ 2464 batches | ms/batch 1490.56 | s_loss  0.68  0.71 | w_loss  0.53  0.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  33 | valid loss  1.50  1.17 | lr 0.00177|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |     0/ 2464 batches | ms/batch 49.17 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  34 |   200/ 2464 batches | ms/batch 1399.80 | s_loss  0.70  0.73 | w_loss  0.56  0.57\n",
      "| epoch  34 |   400/ 2464 batches | ms/batch 1425.80 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  34 |   600/ 2464 batches | ms/batch 1454.44 | s_loss  0.70  0.73 | w_loss  0.55  0.57\n",
      "| epoch  34 |   800/ 2464 batches | ms/batch 1502.58 | s_loss  0.69  0.70 | w_loss  0.53  0.54\n",
      "| epoch  34 |  1000/ 2464 batches | ms/batch 1505.24 | s_loss  0.67  0.69 | w_loss  0.54  0.55\n",
      "| epoch  34 |  1200/ 2464 batches | ms/batch 1484.84 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  34 |  1400/ 2464 batches | ms/batch 1510.05 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  34 |  1600/ 2464 batches | ms/batch 1511.05 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  34 |  1800/ 2464 batches | ms/batch 1503.02 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  34 |  2000/ 2464 batches | ms/batch 1505.76 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  34 |  2200/ 2464 batches | ms/batch 1487.96 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  34 |  2400/ 2464 batches | ms/batch 1509.15 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  34 | valid loss  1.52  1.22 | lr 0.00174|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |     0/ 2464 batches | ms/batch 48.90 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  35 |   200/ 2464 batches | ms/batch 1396.94 | s_loss  0.68  0.70 | w_loss  0.54  0.54\n",
      "| epoch  35 |   400/ 2464 batches | ms/batch 1431.51 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  35 |   600/ 2464 batches | ms/batch 1472.12 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  35 |   800/ 2464 batches | ms/batch 1523.95 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  35 |  1000/ 2464 batches | ms/batch 1491.67 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  35 |  1200/ 2464 batches | ms/batch 1511.17 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  35 |  1400/ 2464 batches | ms/batch 1480.69 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  35 |  1600/ 2464 batches | ms/batch 1504.39 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  35 |  1800/ 2464 batches | ms/batch 1497.56 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  35 |  2000/ 2464 batches | ms/batch 1535.94 | s_loss  0.71  0.73 | w_loss  0.56  0.58\n",
      "| epoch  35 |  2200/ 2464 batches | ms/batch 1510.31 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  35 |  2400/ 2464 batches | ms/batch 1515.00 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  35 | valid loss  1.49  1.21 | lr 0.00174|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  36 |     0/ 2464 batches | ms/batch 44.20 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  36 |   200/ 2464 batches | ms/batch 1298.81 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  36 |   400/ 2464 batches | ms/batch 1301.01 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  36 |   600/ 2464 batches | ms/batch 1394.69 | s_loss  0.68  0.70 | w_loss  0.55  0.55\n",
      "| epoch  36 |   800/ 2464 batches | ms/batch 1354.67 | s_loss  0.68  0.71 | w_loss  0.55  0.57\n",
      "| epoch  36 |  1000/ 2464 batches | ms/batch 1315.45 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  36 |  1200/ 2464 batches | ms/batch 1306.70 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  36 |  1400/ 2464 batches | ms/batch 1293.27 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import GradScaler\n",
    "import re\n",
    "UPDATE_INTERVAL = 200\n",
    "from torch.amp import autocast\n",
    "\n",
    "def next_train(dataloader, cnn_model, rnn_model, batch_size,\n",
    "          labels, optimizer, epoch, ixtoword, image_dir, scaler):\n",
    "    # dataloader : 데이터셋을 불러오는 DataLoader 객체\n",
    "    # labels : 배치 크기만큼의 정수 라벨 텐서 (0 ~ batch_size -1)\n",
    "    # ixtoword : 인덱스를 단어로 변환하는 딕셔너리\n",
    "    # image__dir : 학습 중 생성된 attentio map 이미지를 저장할 디렉토리\n",
    "\n",
    "    cnn_model.train() # 모델이 학습 모드로 설정됨 → Dropout과 같이 학습 중에만 활성화되는 연산이 실행됨됨\n",
    "    rnn_model.train()\n",
    "    s_total_loss0 = 0\n",
    "    s_total_loss1 = 0\n",
    "    w_total_loss0 = 0\n",
    "    w_total_loss1 = 0\n",
    "    count = (epoch + 1) * len(dataloader) # 학습된 데이터 샘플 개수를 추적\n",
    "    start_time = time.time() # 학습 속도를 측정하기 위한 시작 시간 저장\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        # dataloader에서 데이터를 하나씩 꺼내 학습을 진행\n",
    "\n",
    "        # print('step', step)\n",
    "        # 이전 step의 그래디언트 값을 초기화화\n",
    "        rnn_model.zero_grad()\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "        # prepare_data를 이용하여 데이터 정렬\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            words_features, sent_code = cnn_model(imgs[-1]) # CNN 인코더 적용\n",
    "            nef, att_sze = words_features.size(1), words_features.size(2)\n",
    "            hidden = rnn_model.init_hidden(batch_size) # RNN 인코더 적용\n",
    "            words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "            w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
    "                                                 cap_lens, class_ids, batch_size)\n",
    "            w_total_loss0 += w_loss0.item()\n",
    "            w_total_loss1 += w_loss1.item()\n",
    "            loss = w_loss0 + w_loss1\n",
    "\n",
    "            s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "            loss += s_loss0 + s_loss1\n",
    "            s_total_loss0 += s_loss0.item()\n",
    "            s_total_loss1 += s_loss1.item()\n",
    "\n",
    "        scaler.scale(loss).backward()  # loss에 대해 그래디언트를 계산하고 스케일링\n",
    "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), cfg.TRAIN.RNN_GRAD_CLIP)\n",
    "        \n",
    "            # optimizer.step() 전에 scaler.step()을 호출하여 가중치를 업데이트\n",
    "        scaler.step(optimizer)  # 가중치 업데이트\n",
    "        scaler.update()  # 스케일러 업데이트\n",
    "\n",
    "        if step % UPDATE_INTERVAL == 0:\n",
    "                # 일정한 간격마다 손실값 및 학습 속도 출력\n",
    "            count = epoch * len(dataloader) + step\n",
    "\n",
    "            s_cur_loss0 = s_total_loss0 / UPDATE_INTERVAL\n",
    "            s_cur_loss1 = s_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            w_cur_loss0 = w_total_loss0 / UPDATE_INTERVAL\n",
    "            w_cur_loss1 = w_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    's_loss {:5.2f} {:5.2f} | '\n",
    "                    'w_loss {:5.2f} {:5.2f}'\n",
    "                    .format(epoch, step, len(dataloader),\n",
    "                            elapsed * 1000. / UPDATE_INTERVAL,\n",
    "                            s_cur_loss0, s_cur_loss1,\n",
    "                            w_cur_loss0, w_cur_loss1))\n",
    "            s_total_loss0 = 0\n",
    "            s_total_loss1 = 0\n",
    "            w_total_loss0 = 0\n",
    "            w_total_loss1 = 0\n",
    "            start_time = time.time()\n",
    "                # attention Maps\n",
    "            img_set, _ = \\\n",
    "                    build_super_images(imgs[-1].cpu(), captions, ixtoword, attn_maps, att_sze)\n",
    "            \"\"\"\n",
    "            build_super_images : 실제 이미지, 어텐션 맵, 캡션을 기반으로 시각적 주의를 시각화한 이미지를 생성하는 역할\n",
    "            텍스트 설명과 함께 이미지와 attention map을 한 장의 이미지로 구성.\n",
    "            img_set : attention map이 적용된 이미지가 저장됨\n",
    "            _ : 두 번째 반환값이 있지만 사용되지 않음을 의미\n",
    "            imgs[-1].cpu() : 마지막 단계의 이미지 텐서를 CPU로 이동시킴\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # 이미지가 유효한 경우 실행\n",
    "            if img_set is not None:\n",
    "                im = Image.fromarray(img_set)\n",
    "                # Numpy 배열을 PIL 이미지 객체로 변환\n",
    "                image_dir_per_epoch = os.path.join(image_dir, str(epoch))\n",
    "                mkdir_p(image_dir_per_epoch)\n",
    "                fullpath = '%s/attention_maps_step%d.png' % (image_dir_per_epoch, step)\n",
    "                # 저장 경로 설정, step : 현재 학습 단계(step) 번호\n",
    "                im.save(fullpath)\n",
    "    return count\n",
    "\n",
    "\n",
    "def next_evaluate(dataloader, cnn_model, rnn_model, batch_size, labels):\n",
    "    cnn_model.eval() # 모델이 평가 모드로 설정됨\n",
    "    rnn_model.eval()\n",
    "    s_total_loss = 0\n",
    "    w_total_loss = 0\n",
    "\n",
    "    with autocast(device_type='cuda'):\n",
    "        for step, data in enumerate(dataloader, 0):\n",
    "            real_imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "\n",
    "            words_features, sent_code = cnn_model(real_imgs[-1])\n",
    "            # nef = words_features.size(1)\n",
    "            # words_features = words_features.view(batch_size, nef, -1)\n",
    "\n",
    "            hidden = rnn_model.init_hidden(batch_size)\n",
    "            words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "            w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
    "                                                cap_lens, class_ids, batch_size)\n",
    "            w_total_loss += (w_loss0 + w_loss1).data\n",
    "\n",
    "            s_loss0, s_loss1 = \\\n",
    "                sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "            s_total_loss += (s_loss0 + s_loss1).data\n",
    "\n",
    "            if step == 50:\n",
    "                break\n",
    "\n",
    "    s_cur_loss = s_total_loss.item() / step\n",
    "    w_cur_loss = w_total_loss.item() / step\n",
    "\n",
    "    return s_cur_loss, w_cur_loss\n",
    "\n",
    "def build_next_models(dataset, batch_size):\n",
    "    # build model ############################################################\n",
    "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "    # RNN_ENCODER : 텍스트 데이터를 인코딩하는 RNN 모델\n",
    "    # n_words : 단어 크기 (텍스트에 포함된 전체 단어\n",
    "    # cfg.TEXT.EMBEDDING_DIM = 256, RNN의 은닉 상태 크기\n",
    "\n",
    "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
    "    # CNN_ENCODER : 이미지를 인코딩하는 CNN 모델\n",
    "    # 텍스 임베딩 차원을 입력으로 받음 → 나중에 텍스트와 이미지 특징을 비교하기 위해해\n",
    "\n",
    "    # labels = Variable(torch.LongTensor(range(batch_size)))\n",
    "    labels = torch.arange(batch_size, dtype=torch.long)\n",
    "    # 배치 크기만큼의 정수 라벨 텐서를 생성 : [0, 1, 2, ..., batch_size -1]\n",
    "    # 모델 학습 시, 배치 내 각 샘플의 인덱스를 나타내는 역할\n",
    "\n",
    "    start_epoch = 0\n",
    "    model_path = '../output/coco_DAMSM_2025_02_19_00_04_58/Model/text_encoder10.pth'\n",
    "    state_dict = torch.load(model_path)\n",
    "    text_encoder.load_state_dict(state_dict)\n",
    "    print('Load ', model_path)\n",
    "    \n",
    "\n",
    "    name = '../output/coco_DAMSM_2025_02_19_00_04_58/Model/image_encoder10.pth'\n",
    "    state_dict = torch.load(name)\n",
    "    image_encoder.load_state_dict(state_dict)\n",
    "    print('Load ', name)\n",
    "    \n",
    "    model_filename = model_path.split('/')[-1]\n",
    "    epoch_str = re.search(r'\\d+', model_filename).group()  # 첫 번째 숫자 추출\n",
    "    start_epoch = int(epoch_str) + 1  # 다음 epoch 번호\n",
    "\n",
    "    print('start_epoch', start_epoch)\n",
    "\n",
    "    if cfg.CUDA:\n",
    "        # cfg.CUDA == True라면, GPU에서 모델을 실행하도록 .cuda()를 호출\n",
    "        text_encoder = text_encoder.cuda()\n",
    "        image_encoder = image_encoder.cuda()\n",
    "        labels = labels.cuda() # 배치별 정수 라벨 텐서\n",
    "        \n",
    "    return text_encoder, image_encoder, labels, start_epoch\n",
    "\n",
    "\n",
    "text_encoder, image_encoder, labels, start_epoch = build_next_models(dataset_train, batch_size)\n",
    "scaler = GradScaler()\n",
    "\n",
    "para = list(text_encoder.parameters())\n",
    "# text_encoder의 모든 학습 가능 파라미터를 리스트에 추가\n",
    "for v in image_encoder.parameters():\n",
    "    if v.requires_grad:\n",
    "        # requires_grad=False인 경우 역전파에서 제외시킴\n",
    "        para.append(v)\n",
    "# optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "\n",
    "# 가장 낮은 검증 손실을 저장\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# 옵티마이저를 한 번만 생성\n",
    "try:\n",
    "    lr = cfg.TRAIN.ENCODER_LR # 초기 학습률(lr) 설정\n",
    "    for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
    "        # start_epoch부터 MAX_EPOCH까지 학습 반복\n",
    "        # optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
    "        # Adam 옵티마이저를 사용하여 text/image_encoder를 학습\n",
    "        # 일반적으로는 betas=(0.9, 0.999)를 이용\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        count = next_train(dataloader_train, image_encoder, text_encoder,\n",
    "                    batch_size, labels, optimizer, epoch,\n",
    "                    dataset_train.ixtoword, image_dir, scaler)\n",
    "        # 모델을 한 epoch 동안 학습하는 역할을 함\n",
    "        print('-' * 89)\n",
    "\n",
    "        if len(dataloader_val) > 0:\n",
    "            # 학습한 모델을 검증 데이터에서 평가\n",
    "            # 가중치를 업데이트하지 않고, 현재 모델이 얼마나 잘 동작하는지 평가가\n",
    "            s_loss, w_loss = next_evaluate(dataloader_val, image_encoder,\n",
    "                                      text_encoder, batch_size, labels)\n",
    "            val_loss = s_loss + w_loss # 검증 손실 합산\n",
    "            print('| end epoch {:3d} | valid loss {:5.2f} {:5.2f} | lr {:.5f}|'\n",
    "                  .format(epoch, s_loss, w_loss, scheduler.get_last_lr()[0]))\n",
    "            \n",
    "            #### 가장 낮은 검증 손실을 가진 모델 저장\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(image_encoder.state_dict(),\n",
    "                           f'{model_dir}/best_image_encoder.pth')\n",
    "                torch.save(text_encoder.state_dict(),\n",
    "                           f'{model_dir}/best_text_encoder.pth')\n",
    "                print('Best model saved!')\n",
    "            \n",
    "        print('-' * 89)\n",
    "        \n",
    "        scheduler.step() # 스케줄러로 학습률 감소\n",
    "\n",
    "        #if lr > cfg.TRAIN.ENCODER_LR/10.:\n",
    "            # 현재 학습률이 초기 학습률의 1/10 이상이면 lr를 98%로 감소시킴\n",
    "        #    lr *= 0.98\n",
    "\n",
    "        # 모델 저장\n",
    "        if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n",
    "            epoch == cfg.TRAIN.MAX_EPOCH):\n",
    "            # 일정 epoch마다 또는 MAX_EPOCH이 되면 가중치 저장\n",
    "            torch.save(image_encoder.state_dict(),\n",
    "                       '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
    "            torch.save(text_encoder.state_dict(),\n",
    "                       '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
    "            print('Save models.')\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
      "Load  ../output/coco_DAMSM_2025_02_19_00_04_58/Model/text_encoder35.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blues\\AppData\\Local\\Temp\\ipykernel_16608\\2422545814.py:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "C:\\Users\\blues\\AppData\\Local\\Temp\\ipykernel_16608\\2422545814.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  ../output/coco_DAMSM_2025_02_19_00_04_58/Model/image_encoder35.pth\n",
      "start_epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  36 |     0/ 2464 batches | ms/batch 49.69 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  36 |   200/ 2464 batches | ms/batch 1194.45 | s_loss  0.69  0.72 | w_loss  0.54  0.57\n",
      "| epoch  36 |   400/ 2464 batches | ms/batch 1263.32 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  36 |   600/ 2464 batches | ms/batch 1228.29 | s_loss  0.70  0.73 | w_loss  0.57  0.58\n",
      "| epoch  36 |   800/ 2464 batches | ms/batch 1239.61 | s_loss  0.70  0.73 | w_loss  0.56  0.57\n",
      "| epoch  36 |  1000/ 2464 batches | ms/batch 1306.84 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  36 |  1200/ 2464 batches | ms/batch 1343.51 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  36 |  1400/ 2464 batches | ms/batch 1322.38 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  36 |  1600/ 2464 batches | ms/batch 1429.39 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  36 |  1800/ 2464 batches | ms/batch 1479.87 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  36 |  2000/ 2464 batches | ms/batch 1548.76 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  36 |  2200/ 2464 batches | ms/batch 1585.27 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  36 |  2400/ 2464 batches | ms/batch 1586.20 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  36 | valid loss  1.53  1.22 | lr 0.00200|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blues\\anaconda3\\envs\\2024WINTERSIG\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  37 |     0/ 2464 batches | ms/batch 45.21 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  37 |   200/ 2464 batches | ms/batch 1201.41 | s_loss  0.68  0.71 | w_loss  0.53  0.55\n",
      "| epoch  37 |   400/ 2464 batches | ms/batch 1185.24 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  37 |   600/ 2464 batches | ms/batch 1197.56 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  37 |   800/ 2464 batches | ms/batch 1233.85 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  37 |  1000/ 2464 batches | ms/batch 1254.42 | s_loss  0.68  0.71 | w_loss  0.55  0.57\n",
      "| epoch  37 |  1200/ 2464 batches | ms/batch 1301.68 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  37 |  1400/ 2464 batches | ms/batch 1344.66 | s_loss  0.70  0.73 | w_loss  0.55  0.57\n",
      "| epoch  37 |  1600/ 2464 batches | ms/batch 1397.34 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  37 |  1800/ 2464 batches | ms/batch 1538.97 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  37 |  2000/ 2464 batches | ms/batch 1653.87 | s_loss  0.69  0.72 | w_loss  0.55  0.58\n",
      "| epoch  37 |  2200/ 2464 batches | ms/batch 1653.60 | s_loss  0.71  0.73 | w_loss  0.57  0.58\n",
      "| epoch  37 |  2400/ 2464 batches | ms/batch 1650.45 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  37 | valid loss  1.51  1.24 | lr 0.00200|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |     0/ 2464 batches | ms/batch 52.93 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  38 |   200/ 2464 batches | ms/batch 1738.91 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  38 |   400/ 2464 batches | ms/batch 1703.22 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  38 |   600/ 2464 batches | ms/batch 1735.61 | s_loss  0.67  0.70 | w_loss  0.53  0.54\n",
      "| epoch  38 |   800/ 2464 batches | ms/batch 1690.51 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  38 |  1000/ 2464 batches | ms/batch 1639.39 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  38 |  1200/ 2464 batches | ms/batch 1628.46 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  38 |  1400/ 2464 batches | ms/batch 1594.45 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  38 |  1600/ 2464 batches | ms/batch 1564.63 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  38 |  1800/ 2464 batches | ms/batch 1643.17 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  38 |  2000/ 2464 batches | ms/batch 1701.31 | s_loss  0.68  0.71 | w_loss  0.52  0.54\n",
      "| epoch  38 |  2200/ 2464 batches | ms/batch 1664.67 | s_loss  0.68  0.71 | w_loss  0.55  0.57\n",
      "| epoch  38 |  2400/ 2464 batches | ms/batch 1675.10 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  38 | valid loss  1.49  1.19 | lr 0.00200|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |     0/ 2464 batches | ms/batch 47.33 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  39 |   200/ 2464 batches | ms/batch 1281.56 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  39 |   400/ 2464 batches | ms/batch 1303.04 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  39 |   600/ 2464 batches | ms/batch 12831.72 | s_loss  0.68  0.70 | w_loss  0.56  0.57\n",
      "| epoch  39 |   800/ 2464 batches | ms/batch 2814.30 | s_loss  0.70  0.73 | w_loss  0.56  0.58\n",
      "| epoch  39 |  1000/ 2464 batches | ms/batch 2909.68 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  39 |  1200/ 2464 batches | ms/batch 2931.36 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  39 |  1400/ 2464 batches | ms/batch 2890.79 | s_loss  0.68  0.70 | w_loss  0.53  0.56\n",
      "| epoch  39 |  1600/ 2464 batches | ms/batch 2947.53 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  39 |  1800/ 2464 batches | ms/batch 3048.01 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  39 |  2000/ 2464 batches | ms/batch 3159.84 | s_loss  0.71  0.73 | w_loss  0.56  0.58\n",
      "| epoch  39 |  2200/ 2464 batches | ms/batch 3303.07 | s_loss  0.69  0.72 | w_loss  0.54  0.57\n",
      "| epoch  39 |  2400/ 2464 batches | ms/batch 3471.34 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  39 | valid loss  1.52  1.29 | lr 0.00200|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |     0/ 2464 batches | ms/batch 79.58 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  40 |   200/ 2464 batches | ms/batch 2612.26 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  40 |   400/ 2464 batches | ms/batch 1550.30 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  40 |   600/ 2464 batches | ms/batch 1574.63 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  40 |   800/ 2464 batches | ms/batch 1585.65 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  40 |  1000/ 2464 batches | ms/batch 1553.80 | s_loss  0.68  0.71 | w_loss  0.55  0.57\n",
      "| epoch  40 |  1200/ 2464 batches | ms/batch 1217.67 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  40 |  1400/ 2464 batches | ms/batch 1105.24 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  40 |  1600/ 2464 batches | ms/batch 1189.43 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  40 |  1800/ 2464 batches | ms/batch 1252.18 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  40 |  2000/ 2464 batches | ms/batch 1327.21 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  40 |  2200/ 2464 batches | ms/batch 1407.53 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  40 |  2400/ 2464 batches | ms/batch 1482.48 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  40 | valid loss  1.54  1.25 | lr 0.00200|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  41 |     0/ 2464 batches | ms/batch 40.04 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  41 |   200/ 2464 batches | ms/batch 1207.05 | s_loss  0.69  0.71 | w_loss  0.53  0.55\n",
      "| epoch  41 |   400/ 2464 batches | ms/batch 1698.82 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  41 |   600/ 2464 batches | ms/batch 7193.97 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  41 |   800/ 2464 batches | ms/batch 2932.47 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "| epoch  41 |  1000/ 2464 batches | ms/batch 3016.41 | s_loss  0.68  0.71 | w_loss  0.54  0.57\n",
      "| epoch  41 |  1200/ 2464 batches | ms/batch 2997.92 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  41 |  1400/ 2464 batches | ms/batch 3000.53 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  41 |  1600/ 2464 batches | ms/batch 3130.14 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  41 |  1800/ 2464 batches | ms/batch 2998.25 | s_loss  0.68  0.71 | w_loss  0.56  0.57\n",
      "| epoch  41 |  2000/ 2464 batches | ms/batch 2066.52 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  41 |  2200/ 2464 batches | ms/batch 2234.73 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  41 |  2400/ 2464 batches | ms/batch 2284.51 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  41 | valid loss  1.49  1.19 | lr 0.00196|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |     0/ 2464 batches | ms/batch 64.86 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  42 |   200/ 2464 batches | ms/batch 1454.01 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  42 |   400/ 2464 batches | ms/batch 2426.28 | s_loss  0.68  0.70 | w_loss  0.55  0.56\n",
      "| epoch  42 |   600/ 2464 batches | ms/batch 1475.84 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  42 |   800/ 2464 batches | ms/batch 186941.02 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  42 |  1000/ 2464 batches | ms/batch 1434.07 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  42 |  1200/ 2464 batches | ms/batch 1393.29 | s_loss  0.70  0.72 | w_loss  0.57  0.58\n",
      "| epoch  42 |  1400/ 2464 batches | ms/batch 1401.89 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  42 |  1600/ 2464 batches | ms/batch 1441.20 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  42 |  1800/ 2464 batches | ms/batch 1466.84 | s_loss  0.70  0.72 | w_loss  0.55  0.55\n",
      "| epoch  42 |  2000/ 2464 batches | ms/batch 1506.37 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  42 |  2200/ 2464 batches | ms/batch 1542.61 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  42 |  2400/ 2464 batches | ms/batch 1433.81 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  42 | valid loss  1.55  1.25 | lr 0.00196|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |     0/ 2464 batches | ms/batch 39.62 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  43 |   200/ 2464 batches | ms/batch 1020.35 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  43 |   400/ 2464 batches | ms/batch 1014.84 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  43 |   600/ 2464 batches | ms/batch 1417.40 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  43 |   800/ 2464 batches | ms/batch 1321.40 | s_loss  0.67  0.69 | w_loss  0.54  0.55\n",
      "| epoch  43 |  1000/ 2464 batches | ms/batch 1417.46 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  43 |  1200/ 2464 batches | ms/batch 5122.18 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  43 |  1400/ 2464 batches | ms/batch 2966.82 | s_loss  0.70  0.73 | w_loss  0.56  0.58\n",
      "| epoch  43 |  1600/ 2464 batches | ms/batch 2274.92 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  43 |  1800/ 2464 batches | ms/batch 2091.94 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  43 |  2000/ 2464 batches | ms/batch 2060.77 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  43 |  2200/ 2464 batches | ms/batch 1869.41 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  43 |  2400/ 2464 batches | ms/batch 9161.15 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  43 | valid loss  1.51  1.25 | lr 0.00196|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |     0/ 2464 batches | ms/batch 86.37 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  44 |   200/ 2464 batches | ms/batch 2593.76 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  44 |   400/ 2464 batches | ms/batch 2655.15 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  44 |   600/ 2464 batches | ms/batch 2697.71 | s_loss  0.70  0.73 | w_loss  0.55  0.58\n",
      "| epoch  44 |   800/ 2464 batches | ms/batch 2824.50 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  44 |  1000/ 2464 batches | ms/batch 2632.62 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  44 |  1200/ 2464 batches | ms/batch 1092.26 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  44 |  1400/ 2464 batches | ms/batch 1547.97 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  44 |  1600/ 2464 batches | ms/batch 1633.39 | s_loss  0.67  0.69 | w_loss  0.54  0.55\n",
      "| epoch  44 |  1800/ 2464 batches | ms/batch 1644.89 | s_loss  0.69  0.71 | w_loss  0.55  0.55\n",
      "| epoch  44 |  2000/ 2464 batches | ms/batch 1577.94 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  44 |  2200/ 2464 batches | ms/batch 1555.48 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  44 |  2400/ 2464 batches | ms/batch 1596.45 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  44 | valid loss  1.47  1.21 | lr 0.00196|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |     0/ 2464 batches | ms/batch 46.94 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  45 |   200/ 2464 batches | ms/batch 1255.16 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  45 |   400/ 2464 batches | ms/batch 1246.92 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  45 |   600/ 2464 batches | ms/batch 1246.30 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  45 |   800/ 2464 batches | ms/batch 1266.59 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  45 |  1000/ 2464 batches | ms/batch 1251.33 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  45 |  1200/ 2464 batches | ms/batch 1261.46 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  45 |  1400/ 2464 batches | ms/batch 1236.91 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  45 |  1600/ 2464 batches | ms/batch 1239.18 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  45 |  1800/ 2464 batches | ms/batch 1258.35 | s_loss  0.70  0.73 | w_loss  0.55  0.56\n",
      "| epoch  45 |  2000/ 2464 batches | ms/batch 1333.28 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  45 |  2200/ 2464 batches | ms/batch 1475.32 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  45 |  2400/ 2464 batches | ms/batch 1539.00 | s_loss  0.70  0.72 | w_loss  0.56  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  45 | valid loss  1.48  1.20 | lr 0.00196|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  46 |     0/ 2464 batches | ms/batch 44.69 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  46 |   200/ 2464 batches | ms/batch 1008.23 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  46 |   400/ 2464 batches | ms/batch 1057.52 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  46 |   600/ 2464 batches | ms/batch 1057.81 | s_loss  0.69  0.72 | w_loss  0.55  0.56\n",
      "| epoch  46 |   800/ 2464 batches | ms/batch 1101.75 | s_loss  0.70  0.73 | w_loss  0.55  0.57\n",
      "| epoch  46 |  1000/ 2464 batches | ms/batch 1131.24 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  46 |  1200/ 2464 batches | ms/batch 1168.48 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  46 |  1400/ 2464 batches | ms/batch 1199.80 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  46 |  1600/ 2464 batches | ms/batch 1234.69 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  46 |  1800/ 2464 batches | ms/batch 1341.53 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  46 |  2000/ 2464 batches | ms/batch 1450.02 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  46 |  2200/ 2464 batches | ms/batch 1616.98 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  46 |  2400/ 2464 batches | ms/batch 1566.04 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  46 | valid loss  1.52  1.20 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |     0/ 2464 batches | ms/batch 44.97 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  47 |   200/ 2464 batches | ms/batch 1061.62 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  47 |   400/ 2464 batches | ms/batch 1107.83 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  47 |   600/ 2464 batches | ms/batch 1088.34 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  47 |   800/ 2464 batches | ms/batch 1163.01 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  47 |  1000/ 2464 batches | ms/batch 1216.18 | s_loss  0.67  0.70 | w_loss  0.53  0.55\n",
      "| epoch  47 |  1200/ 2464 batches | ms/batch 1238.31 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  47 |  1400/ 2464 batches | ms/batch 1317.73 | s_loss  0.67  0.70 | w_loss  0.54  0.55\n",
      "| epoch  47 |  1600/ 2464 batches | ms/batch 1375.20 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  47 |  1800/ 2464 batches | ms/batch 1507.21 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  47 |  2000/ 2464 batches | ms/batch 1565.37 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  47 |  2200/ 2464 batches | ms/batch 1579.84 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  47 |  2400/ 2464 batches | ms/batch 1579.80 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  47 | valid loss  1.55  1.24 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |     0/ 2464 batches | ms/batch 46.74 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  48 |   200/ 2464 batches | ms/batch 1119.99 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  48 |   400/ 2464 batches | ms/batch 1158.14 | s_loss  0.69  0.71 | w_loss  0.56  0.56\n",
      "| epoch  48 |   600/ 2464 batches | ms/batch 1230.24 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  48 |   800/ 2464 batches | ms/batch 1237.38 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  48 |  1000/ 2464 batches | ms/batch 1271.92 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  48 |  1200/ 2464 batches | ms/batch 1336.54 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  48 |  1400/ 2464 batches | ms/batch 1418.44 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  48 |  1600/ 2464 batches | ms/batch 1495.31 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  48 |  1800/ 2464 batches | ms/batch 1623.64 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  48 |  2000/ 2464 batches | ms/batch 1570.39 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  48 |  2200/ 2464 batches | ms/batch 1571.95 | s_loss  0.67  0.70 | w_loss  0.54  0.55\n",
      "| epoch  48 |  2400/ 2464 batches | ms/batch 1598.04 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  48 | valid loss  1.52  1.27 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |     0/ 2464 batches | ms/batch 46.92 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  49 |   200/ 2464 batches | ms/batch 1165.96 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  49 |   400/ 2464 batches | ms/batch 2876.65 | s_loss  0.71  0.73 | w_loss  0.57  0.59\n",
      "| epoch  49 |   600/ 2464 batches | ms/batch 3509.65 | s_loss  0.68  0.70 | w_loss  0.54  0.55\n",
      "| epoch  49 |   800/ 2464 batches | ms/batch 3093.21 | s_loss  0.68  0.70 | w_loss  0.54  0.56\n",
      "| epoch  49 |  1000/ 2464 batches | ms/batch 3109.78 | s_loss  0.69  0.71 | w_loss  0.56  0.56\n",
      "| epoch  49 |  1200/ 2464 batches | ms/batch 3113.40 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  49 |  1400/ 2464 batches | ms/batch 3078.44 | s_loss  0.68  0.70 | w_loss  0.53  0.55\n",
      "| epoch  49 |  1600/ 2464 batches | ms/batch 3243.24 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  49 |  1800/ 2464 batches | ms/batch 3117.63 | s_loss  0.69  0.71 | w_loss  0.54  0.57\n",
      "| epoch  49 |  2000/ 2464 batches | ms/batch 2297.80 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  49 |  2200/ 2464 batches | ms/batch 2267.13 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  49 |  2400/ 2464 batches | ms/batch 2295.04 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  49 | valid loss  1.53  1.23 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |     0/ 2464 batches | ms/batch 64.59 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  50 |   200/ 2464 batches | ms/batch 1649.64 | s_loss  0.67  0.70 | w_loss  0.54  0.56\n",
      "| epoch  50 |   400/ 2464 batches | ms/batch 1643.87 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  50 |   600/ 2464 batches | ms/batch 1651.26 | s_loss  0.70  0.73 | w_loss  0.55  0.57\n",
      "| epoch  50 |   800/ 2464 batches | ms/batch 1653.91 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  50 |  1000/ 2464 batches | ms/batch 1653.72 | s_loss  0.68  0.70 | w_loss  0.54  0.54\n",
      "| epoch  50 |  1200/ 2464 batches | ms/batch 1747.14 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  50 |  1400/ 2464 batches | ms/batch 1791.91 | s_loss  0.67  0.70 | w_loss  0.54  0.55\n",
      "| epoch  50 |  1600/ 2464 batches | ms/batch 3056.11 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  50 |  1800/ 2464 batches | ms/batch 23588.80 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  50 |  2000/ 2464 batches | ms/batch 3845.69 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "| epoch  50 |  2200/ 2464 batches | ms/batch 3141.87 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  50 |  2400/ 2464 batches | ms/batch 2630.68 | s_loss  0.69  0.71 | w_loss  0.54  0.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  50 | valid loss  1.54  1.27 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save models.\n",
      "| epoch  51 |     0/ 2464 batches | ms/batch 46.72 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  51 |   200/ 2464 batches | ms/batch 2303.92 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  51 |   400/ 2464 batches | ms/batch 2257.70 | s_loss  0.68  0.71 | w_loss  0.54  0.55\n",
      "| epoch  51 |   600/ 2464 batches | ms/batch 2502.18 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  51 |   800/ 2464 batches | ms/batch 2527.68 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  51 |  1000/ 2464 batches | ms/batch 1530.84 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  51 |  1200/ 2464 batches | ms/batch 1431.75 | s_loss  0.67  0.69 | w_loss  0.53  0.54\n",
      "| epoch  51 |  1400/ 2464 batches | ms/batch 1409.88 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  51 |  1600/ 2464 batches | ms/batch 1354.50 | s_loss  0.69  0.72 | w_loss  0.56  0.58\n",
      "| epoch  51 |  1800/ 2464 batches | ms/batch 1394.40 | s_loss  0.70  0.73 | w_loss  0.56  0.58\n",
      "| epoch  51 |  2000/ 2464 batches | ms/batch 1383.21 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  51 |  2200/ 2464 batches | ms/batch 1384.96 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  51 |  2400/ 2464 batches | ms/batch 1377.87 | s_loss  0.70  0.73 | w_loss  0.55  0.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  51 | valid loss  1.52  1.21 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |     0/ 2464 batches | ms/batch 40.63 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  52 |   200/ 2464 batches | ms/batch 1030.73 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  52 |   400/ 2464 batches | ms/batch 1163.35 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  52 |   600/ 2464 batches | ms/batch 1801.47 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  52 |   800/ 2464 batches | ms/batch 1782.88 | s_loss  0.67  0.70 | w_loss  0.53  0.54\n",
      "| epoch  52 |  1000/ 2464 batches | ms/batch 2398.69 | s_loss  0.68  0.71 | w_loss  0.53  0.55\n",
      "| epoch  52 |  1200/ 2464 batches | ms/batch 1986.45 | s_loss  0.69  0.72 | w_loss  0.55  0.57\n",
      "| epoch  52 |  1400/ 2464 batches | ms/batch 1798.52 | s_loss  0.70  0.71 | w_loss  0.54  0.55\n",
      "| epoch  52 |  1600/ 2464 batches | ms/batch 2893.56 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  52 |  1800/ 2464 batches | ms/batch 1987.25 | s_loss  0.68  0.70 | w_loss  0.53  0.54\n",
      "| epoch  52 |  2000/ 2464 batches | ms/batch 1990.12 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  52 |  2200/ 2464 batches | ms/batch 2209.74 | s_loss  0.71  0.73 | w_loss  0.57  0.58\n",
      "| epoch  52 |  2400/ 2464 batches | ms/batch 2951.86 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  52 | valid loss  1.49  1.20 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |     0/ 2464 batches | ms/batch 81.82 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  53 |   200/ 2464 batches | ms/batch 2223.30 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  53 |   400/ 2464 batches | ms/batch 2177.32 | s_loss  0.69  0.71 | w_loss  0.54  0.56\n",
      "| epoch  53 |   600/ 2464 batches | ms/batch 2248.22 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  53 |   800/ 2464 batches | ms/batch 2281.55 | s_loss  0.70  0.72 | w_loss  0.55  0.57\n",
      "| epoch  53 |  1000/ 2464 batches | ms/batch 1553.89 | s_loss  0.68  0.71 | w_loss  0.55  0.56\n",
      "| epoch  53 |  1200/ 2464 batches | ms/batch 1148.36 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  53 |  1400/ 2464 batches | ms/batch 1206.97 | s_loss  0.69  0.71 | w_loss  0.56  0.57\n",
      "| epoch  53 |  1600/ 2464 batches | ms/batch 1267.26 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  53 |  1800/ 2464 batches | ms/batch 1291.78 | s_loss  0.69  0.71 | w_loss  0.54  0.55\n",
      "| epoch  53 |  2000/ 2464 batches | ms/batch 1443.73 | s_loss  0.69  0.71 | w_loss  0.55  0.55\n",
      "| epoch  53 |  2200/ 2464 batches | ms/batch 1559.96 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  53 |  2400/ 2464 batches | ms/batch 1561.18 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  53 | valid loss  1.49  1.20 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |     0/ 2464 batches | ms/batch 48.12 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch  54 |   200/ 2464 batches | ms/batch 1065.58 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  54 |   400/ 2464 batches | ms/batch 1092.18 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  54 |   600/ 2464 batches | ms/batch 1081.58 | s_loss  0.71  0.73 | w_loss  0.56  0.58\n",
      "| epoch  54 |   800/ 2464 batches | ms/batch 1085.77 | s_loss  0.68  0.71 | w_loss  0.54  0.57\n",
      "| epoch  54 |  1000/ 2464 batches | ms/batch 1780.34 | s_loss  0.68  0.71 | w_loss  0.54  0.56\n",
      "| epoch  54 |  1200/ 2464 batches | ms/batch 3166.27 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  54 |  1400/ 2464 batches | ms/batch 1728.04 | s_loss  0.69  0.71 | w_loss  0.55  0.57\n",
      "| epoch  54 |  1600/ 2464 batches | ms/batch 1780.25 | s_loss  0.70  0.72 | w_loss  0.55  0.56\n",
      "| epoch  54 |  1800/ 2464 batches | ms/batch 1902.09 | s_loss  0.70  0.72 | w_loss  0.56  0.58\n",
      "| epoch  54 |  2000/ 2464 batches | ms/batch 1999.20 | s_loss  0.69  0.72 | w_loss  0.56  0.57\n",
      "| epoch  54 |  2200/ 2464 batches | ms/batch 9070.65 | s_loss  0.69  0.71 | w_loss  0.55  0.56\n",
      "| epoch  54 |  2400/ 2464 batches | ms/batch 2281.66 | s_loss  0.69  0.72 | w_loss  0.54  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch  54 | valid loss  1.48  1.16 | lr 0.00188|\n",
      "Best model saved!\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |     0/ 2464 batches | ms/batch 65.02 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import GradScaler\n",
    "import re\n",
    "UPDATE_INTERVAL = 200\n",
    "from torch.amp import autocast\n",
    "from datasets import TextDataset\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "def next_train(dataloader, cnn_model, rnn_model, batch_size,\n",
    "          labels, optimizer, epoch, ixtoword, image_dir, scaler):\n",
    "    # dataloader : 데이터셋을 불러오는 DataLoader 객체\n",
    "    # labels : 배치 크기만큼의 정수 라벨 텐서 (0 ~ batch_size -1)\n",
    "    # ixtoword : 인덱스를 단어로 변환하는 딕셔너리\n",
    "    # image__dir : 학습 중 생성된 attentio map 이미지를 저장할 디렉토리\n",
    "\n",
    "    cnn_model.train() # 모델이 학습 모드로 설정됨 → Dropout과 같이 학습 중에만 활성화되는 연산이 실행됨됨\n",
    "    rnn_model.train()\n",
    "    s_total_loss0 = 0\n",
    "    s_total_loss1 = 0\n",
    "    w_total_loss0 = 0\n",
    "    w_total_loss1 = 0\n",
    "    count = (epoch + 1) * len(dataloader) # 학습된 데이터 샘플 개수를 추적\n",
    "    start_time = time.time() # 학습 속도를 측정하기 위한 시작 시간 저장\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        # dataloader에서 데이터를 하나씩 꺼내 학습을 진행\n",
    "\n",
    "        # print('step', step)\n",
    "        # 이전 step의 그래디언트 값을 초기화화\n",
    "        rnn_model.zero_grad()\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "        # prepare_data를 이용하여 데이터 정렬\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            words_features, sent_code = cnn_model(imgs[-1]) # CNN 인코더 적용\n",
    "            nef, att_sze = words_features.size(1), words_features.size(2)\n",
    "            hidden = rnn_model.init_hidden(batch_size) # RNN 인코더 적용\n",
    "            words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "            w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
    "                                                 cap_lens, class_ids, batch_size)\n",
    "            w_total_loss0 += w_loss0.item()\n",
    "            w_total_loss1 += w_loss1.item()\n",
    "            loss = w_loss0 + w_loss1\n",
    "\n",
    "            s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "            loss += s_loss0 + s_loss1\n",
    "            s_total_loss0 += s_loss0.item()\n",
    "            s_total_loss1 += s_loss1.item()\n",
    "\n",
    "        scaler.scale(loss).backward()  # loss에 대해 그래디언트를 계산하고 스케일링\n",
    "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), cfg.TRAIN.RNN_GRAD_CLIP)\n",
    "        \n",
    "            # optimizer.step() 전에 scaler.step()을 호출하여 가중치를 업데이트\n",
    "        scaler.step(optimizer)  # 가중치 업데이트\n",
    "        scaler.update()  # 스케일러 업데이트\n",
    "\n",
    "        if step % UPDATE_INTERVAL == 0:\n",
    "                # 일정한 간격마다 손실값 및 학습 속도 출력\n",
    "            count = epoch * len(dataloader) + step\n",
    "\n",
    "            s_cur_loss0 = s_total_loss0 / UPDATE_INTERVAL\n",
    "            s_cur_loss1 = s_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            w_cur_loss0 = w_total_loss0 / UPDATE_INTERVAL\n",
    "            w_cur_loss1 = w_total_loss1 / UPDATE_INTERVAL\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    's_loss {:5.2f} {:5.2f} | '\n",
    "                    'w_loss {:5.2f} {:5.2f}'\n",
    "                    .format(epoch, step, len(dataloader),\n",
    "                            elapsed * 1000. / UPDATE_INTERVAL,\n",
    "                            s_cur_loss0, s_cur_loss1,\n",
    "                            w_cur_loss0, w_cur_loss1))\n",
    "            s_total_loss0 = 0\n",
    "            s_total_loss1 = 0\n",
    "            w_total_loss0 = 0\n",
    "            w_total_loss1 = 0\n",
    "            start_time = time.time()\n",
    "                # attention Maps\n",
    "            img_set, _ = \\\n",
    "                    build_super_images(imgs[-1].cpu(), captions, ixtoword, attn_maps, att_sze)\n",
    "            \"\"\"\n",
    "            build_super_images : 실제 이미지, 어텐션 맵, 캡션을 기반으로 시각적 주의를 시각화한 이미지를 생성하는 역할\n",
    "            텍스트 설명과 함께 이미지와 attention map을 한 장의 이미지로 구성.\n",
    "            img_set : attention map이 적용된 이미지가 저장됨\n",
    "            _ : 두 번째 반환값이 있지만 사용되지 않음을 의미\n",
    "            imgs[-1].cpu() : 마지막 단계의 이미지 텐서를 CPU로 이동시킴\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # 이미지가 유효한 경우 실행\n",
    "            if img_set is not None:\n",
    "                im = Image.fromarray(img_set)\n",
    "                # Numpy 배열을 PIL 이미지 객체로 변환\n",
    "                image_dir_per_epoch = os.path.join(image_dir, str(epoch))\n",
    "                mkdir_p(image_dir_per_epoch)\n",
    "                fullpath = '%s/attention_maps_step%d.png' % (image_dir_per_epoch, step)\n",
    "                # 저장 경로 설정, step : 현재 학습 단계(step) 번호\n",
    "                im.save(fullpath)\n",
    "    return count\n",
    "\n",
    "\n",
    "def next_evaluate(dataloader, cnn_model, rnn_model, batch_size, labels):\n",
    "    cnn_model.eval() # 모델이 평가 모드로 설정됨\n",
    "    rnn_model.eval()\n",
    "    s_total_loss = 0\n",
    "    w_total_loss = 0\n",
    "\n",
    "    with autocast(device_type='cuda'):\n",
    "        for step, data in enumerate(dataloader, 0):\n",
    "            real_imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "\n",
    "            words_features, sent_code = cnn_model(real_imgs[-1])\n",
    "            # nef = words_features.size(1)\n",
    "            # words_features = words_features.view(batch_size, nef, -1)\n",
    "\n",
    "            hidden = rnn_model.init_hidden(batch_size)\n",
    "            words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "            w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
    "                                                cap_lens, class_ids, batch_size)\n",
    "            w_total_loss += (w_loss0 + w_loss1).data\n",
    "\n",
    "            s_loss0, s_loss1 = \\\n",
    "                sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "            s_total_loss += (s_loss0 + s_loss1).data\n",
    "\n",
    "            if step == 50:\n",
    "                break\n",
    "\n",
    "    s_cur_loss = s_total_loss.item() / step\n",
    "    w_cur_loss = w_total_loss.item() / step\n",
    "\n",
    "    return s_cur_loss, w_cur_loss\n",
    "\n",
    "def build_next_models(dataset, batch_size):\n",
    "    # build model ############################################################\n",
    "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "    # RNN_ENCODER : 텍스트 데이터를 인코딩하는 RNN 모델\n",
    "    # n_words : 단어 크기 (텍스트에 포함된 전체 단어\n",
    "    # cfg.TEXT.EMBEDDING_DIM = 256, RNN의 은닉 상태 크기\n",
    "\n",
    "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
    "    # CNN_ENCODER : 이미지를 인코딩하는 CNN 모델\n",
    "    # 텍스 임베딩 차원을 입력으로 받음 → 나중에 텍스트와 이미지 특징을 비교하기 위해해\n",
    "\n",
    "    # labels = Variable(torch.LongTensor(range(batch_size)))\n",
    "    labels = torch.arange(batch_size, dtype=torch.long)\n",
    "    # 배치 크기만큼의 정수 라벨 텐서를 생성 : [0, 1, 2, ..., batch_size -1]\n",
    "    # 모델 학습 시, 배치 내 각 샘플의 인덱스를 나타내는 역할\n",
    "\n",
    "    start_epoch = 0\n",
    "    model_path = '../output/coco_DAMSM_2025_02_19_00_04_58/Model/text_encoder35.pth'\n",
    "    state_dict = torch.load(model_path)\n",
    "    text_encoder.load_state_dict(state_dict)\n",
    "    print('Load ', model_path)\n",
    "    \n",
    "\n",
    "    name = '../output/coco_DAMSM_2025_02_19_00_04_58/Model/image_encoder35.pth'\n",
    "    state_dict = torch.load(name)\n",
    "    image_encoder.load_state_dict(state_dict)\n",
    "    print('Load ', name)\n",
    "    \n",
    "    model_filename = model_path.split('/')[-1]\n",
    "    epoch_str = re.search(r'\\d+', model_filename).group()  # 첫 번째 숫자 추출\n",
    "    start_epoch = int(epoch_str) + 1  # 다음 epoch 번호\n",
    "\n",
    "    print('start_epoch', start_epoch)\n",
    "\n",
    "    if cfg.CUDA:\n",
    "        # cfg.CUDA == True라면, GPU에서 모델을 실행하도록 .cuda()를 호출\n",
    "        text_encoder = text_encoder.cuda()\n",
    "        image_encoder = image_encoder.cuda()\n",
    "        labels = labels.cuda() # 배치별 정수 라벨 텐서\n",
    "        \n",
    "    return text_encoder, image_encoder, labels, start_epoch\n",
    "\n",
    "image_dir ='../output/coco_DAMSM_2025_02_19_00_04_58/Image'\n",
    "model_dir = '../output/coco_DAMSM_2025_02_19_00_04_58/Model'\n",
    "text_encoder, image_encoder, labels, start_epoch = build_next_models(dataset_train, batch_size)\n",
    "scaler = GradScaler()\n",
    "optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.98)\n",
    "\n",
    "para = list(text_encoder.parameters())\n",
    "# text_encoder의 모든 학습 가능 파라미터를 리스트에 추가\n",
    "for v in image_encoder.parameters():\n",
    "    if v.requires_grad:\n",
    "        # requires_grad=False인 경우 역전파에서 제외시킴\n",
    "        para.append(v)\n",
    "# optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "\n",
    "# 가장 낮은 검증 손실을 저장\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# 옵티마이저를 한 번만 생성\n",
    "try:\n",
    "    lr = cfg.TRAIN.ENCODER_LR # 초기 학습률(lr) 설정\n",
    "    for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
    "        # start_epoch부터 MAX_EPOCH까지 학습 반복\n",
    "        # optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
    "        # Adam 옵티마이저를 사용하여 text/image_encoder를 학습\n",
    "        # 일반적으로는 betas=(0.9, 0.999)를 이용\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        count = next_train(dataloader_train, image_encoder, text_encoder,\n",
    "                    batch_size, labels, optimizer, epoch,\n",
    "                    dataset_train.ixtoword, image_dir, scaler)\n",
    "        # 모델을 한 epoch 동안 학습하는 역할을 함\n",
    "        print('-' * 89)\n",
    "\n",
    "        if len(dataloader_val) > 0:\n",
    "            # 학습한 모델을 검증 데이터에서 평가\n",
    "            # 가중치를 업데이트하지 않고, 현재 모델이 얼마나 잘 동작하는지 평가가\n",
    "            s_loss, w_loss = next_evaluate(dataloader_val, image_encoder,\n",
    "                                      text_encoder, batch_size, labels)\n",
    "            val_loss = s_loss + w_loss # 검증 손실 합산\n",
    "            print('| end epoch {:3d} | valid loss {:5.2f} {:5.2f} | lr {:.5f}|'\n",
    "                  .format(epoch, s_loss, w_loss, scheduler.get_last_lr()[0]))\n",
    "            \n",
    "            #### 가장 낮은 검증 손실을 가진 모델 저장\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(image_encoder.state_dict(),\n",
    "                           f'{model_dir}/best_image_encoder.pth')\n",
    "                torch.save(text_encoder.state_dict(),\n",
    "                           f'{model_dir}/best_text_encoder.pth')\n",
    "                print('Best model saved!')\n",
    "            \n",
    "        print('-' * 89)\n",
    "        \n",
    "        scheduler.step() # 스케줄러로 학습률 감소\n",
    "\n",
    "        #if lr > cfg.TRAIN.ENCODER_LR/10.:\n",
    "            # 현재 학습률이 초기 학습률의 1/10 이상이면 lr를 98%로 감소시킴\n",
    "        #    lr *= 0.98\n",
    "\n",
    "        # 모델 저장\n",
    "        if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n",
    "            epoch == cfg.TRAIN.MAX_EPOCH):\n",
    "            # 일정 epoch마다 또는 MAX_EPOCH이 되면 가중치 저장\n",
    "            torch.save(image_encoder.state_dict(),\n",
    "                       '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
    "            torch.save(text_encoder.state_dict(),\n",
    "                       '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
    "            print('Save models.')\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAMSM 모델은 54epoch에서 학습을 마쳤으며, 54epoch이 Best Model Saved이기 때문에 나중에 이어서 학습할 때 Best Model로 이어서 학습 시키기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cfg_file='cfg/coco_atn2.yml', gpu_id=0, data_dir='', manualSeed=None)\n"
     ]
    }
   ],
   "source": [
    "# 여기부턴 GAN 모델 학습 과정\n",
    "sys.argv = ['AttnGAN_train.py', '--cfg', 'cfg/coco_atn2.yml', '--gpu', '0']\n",
    "# coco_atn2.yml\n",
    "args = parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'B_VALIDATION': False,\n",
      " 'CONFIG_NAME': 'glu-gan2',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'coco',\n",
      " 'DATA_DIR': '../data/coco',\n",
      " 'GAN': {'B_ATTENTION': True,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 96,\n",
      "         'GF_DIM': 48,\n",
      "         'R_NUM': 3,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': 0,\n",
      " 'RNN_TYPE': 'LSTM',\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 12},\n",
      " 'TRAIN': {'BATCH_SIZE': 14,\n",
      "           'B_NET_D': True,\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'ENCODER_LR': 0.002,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 50,\n",
      "           'NET_E': '../DAMSMencoders/best_text_encoder.pth',\n",
      "           'NET_G': '',\n",
      "           'RNN_GRAD_CLIP': 0.25,\n",
      "           'SMOOTH': {'GAMMA1': 4.0,\n",
      "                      'GAMMA2': 5.0,\n",
      "                      'GAMMA3': 10.0,\n",
      "                      'LAMBDA': 50.0},\n",
      "           'SNAPSHOT_INTERVAL': 3},\n",
      " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 3},\n",
      " 'WORKERS': 4}\n"
     ]
    }
   ],
   "source": [
    "if args.cfg_file is not None:\n",
    "     cfg_from_file(args.cfg_file)\n",
    "\n",
    "if args.gpu_id == -1:\n",
    "    cfg.CUDA = False\n",
    "else:\n",
    "    cfg.GPU_ID = args.gpu_id\n",
    "\n",
    "if args.data_dir != '':\n",
    "    cfg.DATA_DIR = args.data_dir\n",
    "    # DATA_DIR: '../data/coco'\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manualSeed: None\n",
      "Updated manualSeed: 1164\n"
     ]
    }
   ],
   "source": [
    "print('manualSeed:', args.manualSeed)  # manualSeed 값 출력\n",
    "if not cfg.TRAIN.FLAG:\n",
    "    args.manualSeed = 100\n",
    "elif args.manualSeed is None: # TRAIN.FLAG = true\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "    \n",
    "print('Updated manualSeed:', args.manualSeed)\n",
    "random.seed(args.manualSeed)\n",
    "np.random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if cfg.CUDA:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle file from:  ../data/coco\\voca_dictionary\\captions.pickle\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = '../output/%s_%s_%s' % \\\n",
    "        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "torch.cuda.set_device(cfg.GPU_ID)\n",
    "# 저장된 GPU ID로 GPU를 설정한다.\n",
    "cudnn.benchmark = True\n",
    "# cuDNN의 최적화 활성화화\n",
    "\n",
    "split_dir, bshuffle = 'train', True\n",
    "if not cfg.TRAIN.FLAG:\n",
    "    # bshuffle = False\n",
    "    split_dir = 'test'\n",
    "    # GAN 모델에서는 Validation 데이터 셋을 사용하지 않는 것으로 보임 -> Train만 쓰자!\n",
    "\n",
    "# Get data loader\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(int(imsize * 76 / 64)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "\n",
    "#### train dataset Load \n",
    "dataset_GAN_train = TextDataset(cfg.DATA_DIR, vocadict_dir, 'train',\n",
    "                          base_size=cfg.TREE.BASE_SIZE,\n",
    "                          transform=image_transform)\n",
    "assert dataset_GAN_train, \"Invalid train Dataset\"\n",
    "\n",
    "\n",
    "GAN_dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_GAN_train, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "        drop_last=True, shuffle=bshuffle, num_workers=int(cfg.WORKERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
      "Load image encoder from: ../DAMSMencoders/best_image_encoder.pth\n",
      "Load text encoder from: ../DAMSMencoders/best_text_encoder.pth\n",
      "# of netsD 3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_MultiProcessingDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m start_t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mFLAG:\n\u001b[1;32m---> 58\u001b[0m     \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# cfg.TRAIN.FLAG = FAlSE 인 경우\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# generate images from pre-extracted embeddings\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mB_VALIDATION:\n",
      "File \u001b[1;32mc:\\Users\\blues\\바탕 화면\\Hyewon\\Study\\SIG\\2024-SIG\\2024_winter_sig\\image_processing\\code\\trainer.py:270\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m data_iter = iter(self.data_loader)\n\u001b[0;32m    266\u001b[0m step = 0\n\u001b[0;32m    267\u001b[0m while step < self.num_batches:\n\u001b[0;32m    268\u001b[0m     # reset requires_grad to be trainable for all Ds\n\u001b[0;32m    269\u001b[0m     # self.set_requires_grad_value(netsD, True)\n\u001b[1;32m--> 270\u001b[0m \n\u001b[0;32m    271\u001b[0m     ######################################################\n\u001b[0;32m    272\u001b[0m     # (1) Prepare training data and Compute text embeddings\n\u001b[0;32m    273\u001b[0m     ######################################################\n\u001b[0;32m    274\u001b[0m     data = data_iter.next()\n\u001b[0;32m    275\u001b[0m     imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_MultiProcessingDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "from trainer import condGANTrainer as trainer\n",
    "\n",
    "def gen_example(wordtoix, algo):\n",
    "    '''generate images from example sentences'''\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    filepath = '%s/example_filenames.txt' % (cfg.DATA_DIR)\n",
    "    data_dic = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        filenames = f.read().decode('utf8').split('\\n')\n",
    "        for name in filenames:\n",
    "            if len(name) == 0:\n",
    "                continue\n",
    "            filepath = '%s/%s.txt' % (cfg.DATA_DIR, name)\n",
    "            with open(filepath, \"r\") as f:\n",
    "                print('Load from:', name)\n",
    "                sentences = f.read().decode('utf8').split('\\n')\n",
    "                # a list of indices for a sentence\n",
    "                captions = []\n",
    "                cap_lens = []\n",
    "                for sent in sentences:\n",
    "                    if len(sent) == 0:\n",
    "                        continue\n",
    "                    sent = sent.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                    tokens = tokenizer.tokenize(sent.lower())\n",
    "                    if len(tokens) == 0:\n",
    "                        print('sent', sent)\n",
    "                        continue\n",
    "\n",
    "                    rev = []\n",
    "                    for t in tokens:\n",
    "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "                        if len(t) > 0 and t in wordtoix:\n",
    "                            rev.append(wordtoix[t])\n",
    "                    captions.append(rev)\n",
    "                    cap_lens.append(len(rev))\n",
    "            max_len = np.max(cap_lens)\n",
    "\n",
    "            sorted_indices = np.argsort(cap_lens)[::-1]\n",
    "            cap_lens = np.asarray(cap_lens)\n",
    "            cap_lens = cap_lens[sorted_indices]\n",
    "            cap_array = np.zeros((len(captions), max_len), dtype='int64')\n",
    "            for i in range(len(captions)):\n",
    "                idx = sorted_indices[i]\n",
    "                cap = captions[idx]\n",
    "                c_len = len(cap)\n",
    "                cap_array[i, :c_len] = cap\n",
    "            key = name[(name.rfind('/') + 1):]\n",
    "            data_dic[key] = [cap_array, cap_lens, sorted_indices]\n",
    "    algo.gen_example(data_dic)\n",
    "\n",
    "\n",
    "# Define models and go to train/evaluate\n",
    "algo = trainer(output_dir, GAN_dataloader_train, dataset_GAN_train.n_words, dataset_GAN_train.ixtoword)\n",
    "\n",
    "start_t = time.time()\n",
    "if cfg.TRAIN.FLAG:\n",
    "    algo.train()\n",
    "else: # cfg.TRAIN.FLAG = FAlSE 인 경우\n",
    "    # generate images from pre-extracted embeddings\n",
    "    if cfg.B_VALIDATION:\n",
    "        algo.sampling(split_dir)  # generate images for the whole valid dataset\n",
    "    else: # test\n",
    "        gen_example(dataset_GAN_train.wordtoix, algo)  # generate images for customized captions\n",
    "end_t = time.time()\n",
    "print('Total time for training:', end_t - start_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024WINTERSIG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
